{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "final_3_decembre.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27520015368c47aca22bbb56f4c12252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_154325d3b292473db3580233f35d6d56",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f636175612a64057b8f73c07ef0f36ff",
              "IPY_MODEL_415a13cd01714fa6acf99fda552b0b47",
              "IPY_MODEL_dbf0d15fb44a44cab5bc08313409102b"
            ]
          }
        },
        "154325d3b292473db3580233f35d6d56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f636175612a64057b8f73c07ef0f36ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ccb4984b67c34c909a68d485595a42f2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 33%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5f960769740741b69cedd7b8f9f63d96"
          }
        },
        "415a13cd01714fa6acf99fda552b0b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5701f1cd71dc447a9c4e34b3f56b80fa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 12,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4812723a9dc74ecfba6efcc8aa6dfbfc"
          }
        },
        "dbf0d15fb44a44cab5bc08313409102b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ec7a0946bf524344a84b62401ae18b4b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/12 [16:44&lt;33:25, 250.73s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_203902127bb4410b87fced70afa1747d"
          }
        },
        "ccb4984b67c34c909a68d485595a42f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5f960769740741b69cedd7b8f9f63d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5701f1cd71dc447a9c4e34b3f56b80fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4812723a9dc74ecfba6efcc8aa6dfbfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec7a0946bf524344a84b62401ae18b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "203902127bb4410b87fced70afa1747d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WittyTheMighty/ML-kaggle/blob/main/final_5_decembre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZiJViSwLeJ8"
      },
      "source": [
        "# Final Kaggle competition ML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cffyOyN3hsU",
        "outputId": "676ff1e3-570d-4dd6-9ad6-fa24849c6d25"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meAL21kDHHbV"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s9PMiG1H9mX"
      },
      "source": [
        "This is how we can identify the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq9PVvTaHc_y"
      },
      "source": [
        "#Exemple d'utilisation \n",
        "\n",
        "# import wandb\n",
        "# # At the top of your training script, start a new run\n",
        "# wandb.init(project=\"test-project\", entity=\"xabjuwplb\")\n",
        "# # Capture a dictionary of hyperparameters with config\n",
        "# wandb.config = {\n",
        "#   \"learning_rate\": 0.001,\n",
        "#   \"epochs\": 100,\n",
        "#   \"batch_size\": 128\n",
        "# }\n",
        "\n",
        "# # Log metrics inside your training loop to visualize model performance\n",
        "# wandb.log({\"loss\": loss})\n",
        "\n",
        "# # Optional\n",
        "# wandb.watch(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PZkAi5kFRGZ"
      },
      "source": [
        "###Import: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50NROwOnNqC6"
      },
      "source": [
        "# imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch as torch\n",
        "import torchvision as  tv\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "from matplotlib import cm\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "import csv\n",
        "import scipy as sp\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.utils as vutils\n",
        "device = torch.device('cuda')\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.notebook import tqdm\n",
        "import wandb\n",
        "from sklearn.metrics import f1_score\n",
        "# Ensure deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
        "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
        "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
        "\n",
        "# Device configuration\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y5rJHrRFVOD"
      },
      "source": [
        "######Weigth and biais imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiNOnDapFcsH"
      },
      "source": [
        "To track our model we use weights and biaises a plateform that allow us to track the data in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNFNbttcFaI3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uAeECFFFUxy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4JPyre6L8EO"
      },
      "source": [
        "#PATH = './drive/MyDrive/ML-Kaggle/'\n",
        "PATH = \"/content/drive/MyDrive/ML-Kaggle/\"\n",
        "# PATH = '../data/'\n",
        "\n",
        "#Load\n",
        "x_data = np.array(pkl.load(open(PATH+'x_train.pkl', 'rb')))\n",
        "y_data = np.array(pkl.load(open(PATH+'y_train.pkl', 'rb')))\n",
        "\n",
        "x_test_submission =  np.array(pkl.load(open(PATH+'x_test.pkl', 'rb')))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1c10XknRDX9"
      },
      "source": [
        "x_train = x_data[:10000]\n",
        "y_train = y_data[:10000]\n",
        "\n",
        "#Before submit we test the model on this subsection of the dataset without augmented data\n",
        "x_valid = x_data[10001:]\n",
        "y_valid = y_data[10001:]\n",
        "\n",
        "assert len(x_train) == len(y_train)\n",
        "assert len(x_valid) == len(y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liw-MzMqO8d7"
      },
      "source": [
        "labels = np.unique(y_train)\n",
        "labels = dict(zip(labels, range(len(labels))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvLBaLGxFyYP"
      },
      "source": [
        "labels = np.unique(y_valid)\n",
        "labels=dict(zip(labels, range(len(labels))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEBNTESWXEJo"
      },
      "source": [
        "---\n",
        "\n",
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR-Bj3-NSU7G"
      },
      "source": [
        "####Data augmentation from strach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4i4BXE7PHFR"
      },
      "source": [
        "#First preprocessing step \n",
        "\n",
        "#Utilitary function for geometric preprocessing\n",
        "# def flip_verticaly(X):\n",
        "#     flipped_image = copy.deepcopy(rotate90(X))\n",
        "#     for idx,channel in enumerate(flipped_image):\n",
        "#         flipped_image[idx]= np.fliplr(channel)\n",
        "\n",
        "#     return rotate(flipped_image, 270)\n",
        "\n",
        "\n",
        "def flip_horizontaly(X):\n",
        "    flipped_image = copy.deepcopy(X)\n",
        "    flipped_image= np.fliplr(flipped_image)\n",
        "\n",
        "    return flipped_image\n",
        "\n",
        "def rotate90(X):\n",
        "    rot90_img = copy.deepcopy(X)\n",
        "    rot90_img= np.rot90(rot90_img)\n",
        "    return rot90_img\n",
        "\n",
        "def rotate(X,degree):\n",
        "    rot_img = copy.deepcopy(X)\n",
        "    rot_img= sp.ndimage.rotate(rot_img, degree, reshape=False)\n",
        "    return rot_img\n",
        "\n",
        "#An adaptation from:\n",
        "# https://stackoverflow.com/questions/37119071/scipy-rotate-and-zoom-an-image-without-changing-its-dimensions\n",
        "def clipped_zoom(img, zoom_factor):\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    # For multichannel images we don't want to apply the zoom factor to the RGB\n",
        "    # dimension, so instead we create a tuple of zoom factors, one per array\n",
        "    # dimension, with 1's for any trailing dimensions after the width and height.\n",
        "    zoom_tuple = (zoom_factor,) * 2 + (1,) * (img.ndim - 2)\n",
        "\n",
        "    # Zooming out\n",
        "    if zoom_factor < 1:\n",
        "\n",
        "        # Bounding box of the zoomed-out image within the output array\n",
        "        zh = int(np.round(h * zoom_factor))\n",
        "        zw = int(np.round(w * zoom_factor))\n",
        "        top = (h - zh) // 2\n",
        "        left = (w - zw) // 2\n",
        "\n",
        "        # Zero-padding\n",
        "        out = np.zeros_like(img)\n",
        "        out[top:top+zh, left:left+zw] = sp.ndimage.zoom(img, zoom_tuple)\n",
        "\n",
        "    # Zooming in\n",
        "    elif zoom_factor > 1:\n",
        "\n",
        "        # Bounding box of the zoomed-in region within the input array\n",
        "        zh = int(np.round(h / zoom_factor))\n",
        "        zw = int(np.round(w / zoom_factor))\n",
        "        top = (h - zh) // 2\n",
        "        left = (w - zw) // 2\n",
        "\n",
        "        out = sp.ndimage.zoom(img[top:top+zh, left:left+zw], zoom_tuple)\n",
        "\n",
        "        # `out` might still be slightly larger than `img` due to rounding, so\n",
        "        # trim off any extra pixels at the edges\n",
        "        trim_top = ((out.shape[0] - h) // 2)\n",
        "        trim_left = ((out.shape[1] - w) // 2)\n",
        "        out = out[trim_top:trim_top+h, trim_left:trim_left+w]\n",
        "\n",
        "    # If zoom_factor == 1, just return the input array\n",
        "    else:\n",
        "        out = img\n",
        "    return out\n",
        "\n",
        "def blur(X,blur_factor):\n",
        "    blur_img = copy.deepcopy(X)\n",
        "    blur_img= sp.ndimage.gaussian_filter(blur_img, sigma=blur_factor)\n",
        "    return blur_img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeN0vhmWIM6U"
      },
      "source": [
        "zoom_factors = [1.1, 1.3, 1.5]\n",
        "x_train_zoomed_vertical = np.array([clipped_zoom(x,np.random.choice(zoom_factors)) for x in x_train])\n",
        "x_train_zoomed_horizontal = np.array([rotate(clipped_zoom(rotate90(x),np.random.choice(zoom_factors)),270) for x in x_train])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3BSJ-sUOcVP"
      },
      "source": [
        "x_train_rotated = np.array([rotate(x,round(np.random.random()*360)) for x in x_train])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9NSG6ZuO0a5"
      },
      "source": [
        "x_train_flipped_horizontally = np.array([flip_horizontaly(x) for x in x_train])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOvc4L1-Q4MH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E92H1A0LQ-yX"
      },
      "source": [
        "augmented_x_train = np.concatenate([x_train, x_train_zoomed_horizontal, x_train_zoomed_vertical, x_train_rotated, x_train_flipped_horizontally], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQcpU2gNROs1"
      },
      "source": [
        "augmented_y_train = np.concatenate([y_train]*5)\n",
        "augmented_y_train.shape\n",
        "\n",
        "X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n",
        "y = np.array([0, 1, 2])\n",
        "\n",
        "X_train, y_train= shuffle(augmented_x_train, augmented_y_train, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNgfEHbmRsyL"
      },
      "source": [
        "# SAVE AUGMENTED TRAINING SET\n",
        "pkl.dump(augmented_x_train,open(PATH+'x_train_aug.pkl', 'wb'))\n",
        "pkl.dump(augmented_y_train,open(PATH+'y_train_aug.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8su14mjWSMp2"
      },
      "source": [
        "# LOAD AUGMENTED TRAINING SET\n",
        "x_train = pkl.load(open(PATH+'x_train_aug.pkl', 'rb'))\n",
        "y_train = pkl.load(open(PATH+'y_train_aug.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB5RSJ6UuE41"
      },
      "source": [
        "###Normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szwqrVXaLJB0"
      },
      "source": [
        "From strach not used yet. We use pytorch normalize function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2UsbeWttv6y"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "def standardize(data):\n",
        "    standardized_data = []\n",
        "    scaler = StandardScaler()\n",
        "    for img in data:\n",
        "        scaler = scaler.fit(img)\n",
        "        standadized_data.append(scaler.transform(img))\n",
        "    return np.array(standardized_data)\n",
        "\n",
        "def normalize(data):\n",
        "    normalized_data = []\n",
        "    normalizer = Normalizer()\n",
        "    for img in data:\n",
        "        normalizer = normalizer.fit(img)\n",
        "        normalized_data.append(normalizer.transform(img))\n",
        "    return np.array(normalized_data)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-MDKSh4IAXa"
      },
      "source": [
        "\n",
        "# x_train_normalize = normalize(x_train)\n",
        "\n",
        "# #Todo : Normaliser les données de tests\n",
        "\n",
        "# x_test_set = x_train_normalize[:3000]\n",
        "# y_test_set = y_train[:3000]\n",
        "\n",
        "# x_train = x_train_normalize[3000:]\n",
        "# y_train = y_train[3000:]\n",
        "\n",
        "# assert y_train.shape[0] == x_train.shape[0]\n",
        "# assert y_test_set.shape[0] == x_test_set.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be_PY1e8XJ2r"
      },
      "source": [
        "---\n",
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfszZAClLPtK"
      },
      "source": [
        "This is all the model we used in our expirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKdxRj-W3iF_"
      },
      "source": [
        "##### Inspired AlexNet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ4o50Gldcm5"
      },
      "source": [
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self): \n",
        "        super().__init__()\n",
        "        print(\"AlexNet\")\n",
        "        self.conv1 = nn.Conv2d(1,25,5)\n",
        "        self.batchNorm2d1 = nn.BatchNorm2d(25,momentum=0.95)\n",
        "        # we use the maxpool multiple times, but define it once\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # in_channels = 6 because self.conv1 output 6 channel\n",
        "        self.conv2 = nn.Conv2d(25, 50,5)\n",
        "        self.batchNorm2d2 = nn.BatchNorm2d(50,momentum=0.95)\n",
        "        self.conv3 = nn.Conv2d(50 ,100,3)\n",
        "        self.batchNorm2d3 = nn.BatchNorm2d(100,momentum=0.95)\n",
        "        # 5*5 comes from the dimension of the last convnet layer\n",
        "        self.fc1 = nn.Linear(100*9*9, 480) \n",
        "        self.fc2 = nn.Linear(480, 240)\n",
        "        self.fc3 = nn.Linear(240, 120)\n",
        "        self.fc4 = nn.Linear(120, 11)\n",
        "        \n",
        "    def forward(self, x): \n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.batchNorm2d1(x)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.batchNorm2d2(x)\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.batchNorm2d3(x)\n",
        "        x = x.view(-1, 100*9*9)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)  # no activation on final layer \n",
        "        return x\n",
        "\n",
        "\n",
        "class AlexNetv2(nn.Module): \n",
        "    def __init__(self): \n",
        "        super().__init__()\n",
        "        print(\"AlexNetv2\")\n",
        "        self.convolution = nn.Sequential(\n",
        "            nn.Conv2d(1, 48,7), #in, out, kernel_size, stride,padding\n",
        "            nn.MaxPool2d(2,stride=2), #kernel_size\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(48),\n",
        "            #Layer 2\n",
        "            nn.Conv2d(48, 128,5,stride=2), #(45-5)/2 +1 = 21\n",
        "            nn.MaxPool2d(3,2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128,momemtum=0.),\n",
        "            nn.Conv2d(128, 192, 5,padding=2), \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(192, 192, 3,padding=1), #(39-3+1)/1 =37\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(192, 128, 3, 1), # 35\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, 1), \n",
        "            nn.MaxPool2d(2,stride=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            #3200 = 5 * 5 *128\n",
        "            nn.Linear(128*3*3, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(4096, 11),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x): \n",
        "        x = self.convolution(x)\n",
        "        x = x.view(-1,128*3*3)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHQ4J6Hx13C9"
      },
      "source": [
        "#### Inspired ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4J-eiBj158y"
      },
      "source": [
        "class block(nn.Module):\n",
        "        def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
        "            super(block, self).__init__()\n",
        "            self.expansion = 4\n",
        "            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "            self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "            self.bn2 = nn.Batch+ample = identity_downsample\n",
        "        \n",
        "        def forward(self, x):\n",
        "            identity = x\n",
        "            \n",
        "            x = self.conv1(x)\n",
        "            x = self.bn1(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.conv2(x)\n",
        "            x = self.bn2(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.conv3(x)\n",
        "            x = self.bn3(x)\n",
        "            \n",
        "            if self.identity_downsample is not None:\n",
        "                 identity = self.identity_downsample(identity)\n",
        "                    \n",
        "            x += identity\n",
        "            x = self.relu(x)\n",
        "            return x\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # ResNet layers\n",
        "        self.layer1 = self._make_layer_(block, layers[0], out_channels=64, stride=1)\n",
        "        self.layer2 = self._make_layer_(block, layers[1], out_channels=128, stride = 2)\n",
        "        self.layer3 = self._make_layer_(block, layers[2], out_channels=256, stride = 2)\n",
        "        self.layer4 = self._make_layer_(block, layers[3], out_channels=512, stride = 2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512*4, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        x = self.avgpool(x) \n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    def _make_layer_(self, block, num_residual_blocks, out_channels, stride):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "        \n",
        "        if stride != 1 or self.in_channels != out_channels * 4:\n",
        "            identity_downsample = nn.Sequential(nn.Conv2d(self.in_channels, out_channels*4, kernel_size=1, stride=stride), nn.BatchNorm2d(out_channels*4))\n",
        "            \n",
        "        layers.append(block(self.in_channels, out_channels, identity_downsample, stride))\n",
        "        self.in_channels = out_channels*4\n",
        "        \n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "def ResNet50(img_channels=1, num_classes=11):\n",
        "    return ResNet(block, [3,4,6,3], img_channels, num_classes)\n",
        "\n",
        "    \n",
        "def ResNet152(img_channels=1, num_classes=11):\n",
        "    return ResNet(block, [3,8,36,3], img_channels, num_classes)\n",
        "\n",
        "def test():\n",
        "        net = ResNet50()\n",
        "        x = torch.randn(2, 1, 96, 96)\n",
        "        y = net(x).to(\"cuda\")\n",
        "        print(y.shape)\n",
        "        \n",
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTUzHLoq3nRt"
      },
      "source": [
        "#### Inspired VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6z-a5Q3Xmx_"
      },
      "source": [
        "# https://debuggercafe.com/implementing-vgg11-from-scratch-using-pytorch/\n",
        "# the VGG11 architecture\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        # convolutional layers \n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        # fully connected linear layers\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(in_features=4608, out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.5),\n",
        "            nn.Linear(in_features=4096, out_features=11)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        # flatten to prepare for the fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x\n",
        "\n",
        "net = VGG()\n",
        "x = torch.randn(2, 1, 96, 96)\n",
        "y = net(x).to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRRuZnpjLIqh"
      },
      "source": [
        "##Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EqagABGLDZ0"
      },
      "source": [
        "For our pipeline we used the pipeline framework weigths and biaises for easy and fast data parameter tracking. All the result of our experiments will be store on the cloud. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksZme17IPrHY"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cuda')\n",
        "#device = torch.device(\"cpu\")\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "class CustomTensorDataset(Dataset):\n",
        "    \"\"\"TensorDataset with support of transforms.\"\"\"\n",
        "    def __init__(self, tensors, transform=None):\n",
        "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
        "        self.tensors = tensors\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.tensors[0][index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        y = self.tensors[1][index]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tensors[0].size(0)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOqyeUJYKa4T"
      },
      "source": [
        "#####Custom TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2bImeQ2veNv"
      },
      "source": [
        "#Creating dataloader object\n",
        "trans = transforms.Compose(\n",
        "    [transforms.Normalize((0.5), (0.5))]\n",
        ")\n",
        "\n",
        "\n",
        "#Training set\n",
        "x_train_tensor = torch.tensor(x_train,  dtype=torch.float32)[:, None, :, :]\n",
        "y_train_tensor = torch.tensor(np.array([labels[y] for y in y_train]),  dtype=torch.long)\n",
        "\n",
        "train_dataset = CustomTensorDataset(tensors=(x_train_tensor, y_train_tensor), transform=trans)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Validation set\n",
        "x_valid_tensor = torch.tensor(x_valid,  dtype=torch.float32)[:, None, :, :]\n",
        "y_valid_tensor = torch.tensor(np.array([labels[y] for y in y_valid]),  dtype=torch.long)\n",
        "\n",
        "valid_dataset = CustomTensorDataset(tensors=(x_valid_tensor, y_valid_tensor), transform=trans)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is8H3rG-UwQn"
      },
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,  pin_memory = True, batch_size=64, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz957W5DNo4z"
      },
      "source": [
        "####Hyperparameter list\n",
        " \n",
        "If new variable added but them in the config file\n",
        "\n",
        "Supported models: AlexNet\n",
        "\n",
        "```\n",
        "\n",
        "hyperparam = {\n",
        "    \"training_data\": train_data,\n",
        "    \"valid_data\": valid_data,\n",
        "\n",
        "    \"model\":\"AlexNet\",\n",
        "    \"learning_rate\":1e-2,\n",
        "    \"batch_size\":64,\n",
        "    \"epochs\": 5,\n",
        "\n",
        "    \"k_folds\" :5,\n",
        "    \"description\":\"Setting up momemtum to 0.95\"\n",
        "    }\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQG92e_caWVh"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALBccyqVKAYd"
      },
      "source": [
        "#This code is an adaptation from Weigth&BiaisesFramework we established a pipeline to track our data : https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb#scrollTo=bZiTlrNkRKzm&uniqifier=1\n",
        "def model_pipeline(hyperparameters,training_data,valid_data,PATH):\n",
        "    \"\"\"\"\n",
        "    \"Hyperparameters need to be a dictionary with all the necessary information\" Wandb librairie will load the data\"\n",
        "    \"into their server.\"\n",
        "    \"\"\"\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(entity=\"xabjuwplb\",project=\"CNN\", config=hyperparameters):\n",
        "\n",
        "        config = wandb.config\n",
        "        train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=config.batch_size, shuffle=True)\n",
        "        valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "        # make the model, data, and optimization problem\n",
        "        model, criterion, optimizer = make(config)\n",
        "        model.to(device)\n",
        "        criterion.to(device)\n",
        "        # and use them to train the model\n",
        "        train(model, train_dataloader,valid_dataloader, criterion, optimizer, config)\n",
        "\n",
        "        # and test its final performance\n",
        "        test(model, valid_dataloader,train_dataloader)\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make(config):\n",
        "\n",
        "    # Make the model\n",
        "    model = initialize_model(config)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(), lr=config.learning_rate)    \n",
        "    return model, criterion, optimizer\n",
        "\n",
        "def make_loaders(config):\n",
        "\n",
        "    return train_dataloader, valid_dataloader\n",
        "\n",
        "\n",
        "def initialize_model(config):\n",
        "    #When implmenting add it to the map\n",
        "    if config.model == \"AlexNet\":\n",
        "        return AlexNet()\n",
        "    elif config.model == \"AlexNetv2\":\n",
        "        return AlexNetv2()\n",
        "\n",
        "def train(model, train_loader,valid_loader, criterion, optimizer, config):\n",
        "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "    # Run training and track with wandb\n",
        "    total_batches = len(train_loader) * config.epochs\n",
        "    example_ct = 0  # number of examples seen\n",
        "    batch_ct = 0\n",
        "    loss = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        for _, (image,label) in enumerate(train_loader):\n",
        "            loss = train_batch(image, label, model, optimizer, criterion)\n",
        "            valid_loss = valid_batch(valid_loader,model,criterion)\n",
        "            example_ct +=  len(image)\n",
        "            batch_ct += 1\n",
        "\n",
        "            # Report metrics every 25th batch\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss,valid_loss, example_ct, epoch)\n",
        "        test(model,valid_loader,train_loader)\n",
        "    \n",
        "\n",
        "#\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    \n",
        "    # Forward pass ➡\n",
        "\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass ⬅\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "def valid_batch(valid_data_load,model,criterion):\n",
        "\n",
        "    valid_loss = 0\n",
        "    valid_image,valid_label = next(iter(valid_data_load))\n",
        "\n",
        "    valid_images, valid_labels = valid_image.to(device), valid_label.to(device)\n",
        "    # Forward pass ➡\n",
        "    outputs = model(valid_images)\n",
        "\n",
        "    valid_loss += criterion(outputs, valid_labels)\n",
        "\n",
        "    return valid_loss\n",
        "\n",
        "\n",
        "\n",
        "def test(model, test_loader,train_loader):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        true_y = np.array([])\n",
        "        predicted_y = np.array([])\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            # predicted_y = np.append(predicted_y,predicted.cpu().detach().tolist())\n",
        "            # true_y = np.append(true_y,labels.cpu().detach().tolist())\n",
        "\n",
        "        # f1_score_micro = f1_score(true_y,predicted_y,average=\"micro\")\n",
        "        print(f\"Accuracy of the model on the {total} \" +\n",
        "              f\"test images: {100 * correct / total}%\")\n",
        "\n",
        "        \n",
        "        wandb.log({\"test_accuracy\": correct / total})\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        true_y =np.array([])\n",
        "        predicted_y = np.array([])\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            # true_y= np.append(true_y,labels.cpu().detach().tolist())\n",
        "            # predicted_y=np.append(predicted_y,predicted.cpu().detach().tolist())    \n",
        "\n",
        "        print(f\"Accuracy of the model on the {total} \" +\n",
        "              f\"training images: {100 * correct / total}%\")\n",
        "        wandb.log({\"training_accuracy\": correct / total})\n",
        "        # f1_score_micro = f1_score(true_y,predicted_y,average=\"micro\")\n",
        "        # print(f\"F1 micro score {total} \" +\n",
        "        #       f\"training images: {f1_score_micro}\")        \n",
        "        # wandb.log({\"training_accuracy\": correct / total})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGhTokAs0r3k"
      },
      "source": [
        "def train_log(loss,valid_loss, example_ct, epoch):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"training_loss\": loss,\"valid_loss\":valid_loss}, step=example_ct)\n",
        "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
        "    print(f\"Validation loss after \" + str(example_ct).zfill(5) + f\" examples: {valid_loss:.3f}\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weutOzrH7eLH"
      },
      "source": [
        "def save_model(epoch,model,optimizer,loss,path):\n",
        "\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"loss\": loss\n",
        "    }, path)\n",
        "###\n",
        "#\n",
        "def load_model(checkpoints_path):\n",
        "    checkpoints = torch.load(PATH)\n",
        "    with wandb.init(entity=\"xabjuwplb\",project=\"CNN\", config=checkpoints):\n",
        "        config = wandb.config \n",
        "        #Initialize\n",
        "        model = initialize_model(config.model)\n",
        "        optimizer = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "        optimizer = optimizer.load_state_dict(config['optimizer_state_dict'])\n",
        "        checkpoint = torch.load(PATH)\n",
        "        model.load_state_dict(config['model_state_dict'])\n",
        "        optimizer.load_state_dict(config['optimizer_state_dict'])\n",
        "        epoch = config['epoch']\n",
        "        loss = config['loss']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pix4N3fsWKch"
      },
      "source": [
        "config = {\n",
        "\n",
        "    \"model\":\"AlexNetv2\",\n",
        "    \"learning_rate\":1e-2,\n",
        "    \n",
        "    \"batch_size\":32,\n",
        "    \"epochs\": 12,\n",
        "    \"description\": \"\"\n",
        "    }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "143VH24uDpjn"
      },
      "source": [
        "### Running the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "27520015368c47aca22bbb56f4c12252",
            "154325d3b292473db3580233f35d6d56",
            "f636175612a64057b8f73c07ef0f36ff",
            "415a13cd01714fa6acf99fda552b0b47",
            "dbf0d15fb44a44cab5bc08313409102b",
            "ccb4984b67c34c909a68d485595a42f2",
            "5f960769740741b69cedd7b8f9f63d96",
            "5701f1cd71dc447a9c4e34b3f56b80fa",
            "4812723a9dc74ecfba6efcc8aa6dfbfc",
            "ec7a0946bf524344a84b62401ae18b4b",
            "203902127bb4410b87fced70afa1747d"
          ]
        },
        "id": "jMrh0Y2MTY2-",
        "outputId": "f932923b-7f04-4762-ab4f-fe30c433dba3"
      },
      "source": [
        "# Build, train and analyze the model with the pipeline\n",
        "model = model_pipeline(config,train_dataset,valid_dataset,\"model.pth\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/xabjuwplb/CNN/runs/duybagic\" target=\"_blank\">glowing-bush-79</a></strong> to <a href=\"https://wandb.ai/xabjuwplb/CNN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNetv2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27520015368c47aca22bbb56f4c12252",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after 00768 examples: 2.394\n",
            "Validation loss after 00768 examples: 2.398\n",
            "Loss after 01568 examples: 2.378\n",
            "Validation loss after 01568 examples: 2.365\n",
            "Loss after 02368 examples: 2.369\n",
            "Validation loss after 02368 examples: 2.383\n",
            "Loss after 03168 examples: 2.390\n",
            "Validation loss after 03168 examples: 2.373\n",
            "Loss after 03968 examples: 2.368\n",
            "Validation loss after 03968 examples: 2.325\n",
            "Loss after 04768 examples: 2.340\n",
            "Validation loss after 04768 examples: 2.359\n",
            "Loss after 05568 examples: 2.378\n",
            "Validation loss after 05568 examples: 2.321\n",
            "Loss after 06368 examples: 2.355\n",
            "Validation loss after 06368 examples: 2.313\n",
            "Loss after 07168 examples: 2.352\n",
            "Validation loss after 07168 examples: 2.331\n",
            "Loss after 07968 examples: 2.323\n",
            "Validation loss after 07968 examples: 2.392\n",
            "Loss after 08768 examples: 2.302\n",
            "Validation loss after 08768 examples: 2.352\n",
            "Loss after 09568 examples: 2.264\n",
            "Validation loss after 09568 examples: 2.343\n",
            "Loss after 10368 examples: 2.335\n",
            "Validation loss after 10368 examples: 2.322\n",
            "Loss after 11168 examples: 2.394\n",
            "Validation loss after 11168 examples: 2.301\n",
            "Loss after 11968 examples: 2.267\n",
            "Validation loss after 11968 examples: 2.314\n",
            "Loss after 12768 examples: 2.358\n",
            "Validation loss after 12768 examples: 2.373\n",
            "Loss after 13568 examples: 2.249\n",
            "Validation loss after 13568 examples: 2.263\n",
            "Loss after 14368 examples: 2.318\n",
            "Validation loss after 14368 examples: 2.275\n",
            "Loss after 15168 examples: 2.317\n",
            "Validation loss after 15168 examples: 2.276\n",
            "Loss after 15968 examples: 2.241\n",
            "Validation loss after 15968 examples: 2.403\n",
            "Loss after 16768 examples: 2.280\n",
            "Validation loss after 16768 examples: 2.235\n",
            "Loss after 17568 examples: 2.273\n",
            "Validation loss after 17568 examples: 2.339\n",
            "Loss after 18368 examples: 2.249\n",
            "Validation loss after 18368 examples: 2.195\n",
            "Loss after 19168 examples: 2.074\n",
            "Validation loss after 19168 examples: 2.108\n",
            "Loss after 19968 examples: 2.199\n",
            "Validation loss after 19968 examples: 2.133\n",
            "Loss after 20768 examples: 2.383\n",
            "Validation loss after 20768 examples: 2.185\n",
            "Loss after 21568 examples: 2.467\n",
            "Validation loss after 21568 examples: 2.223\n",
            "Loss after 22368 examples: 2.121\n",
            "Validation loss after 22368 examples: 2.283\n",
            "Loss after 23168 examples: 2.257\n",
            "Validation loss after 23168 examples: 2.170\n",
            "Loss after 23968 examples: 2.105\n",
            "Validation loss after 23968 examples: 2.160\n",
            "Loss after 24768 examples: 2.215\n",
            "Validation loss after 24768 examples: 2.141\n",
            "Loss after 25568 examples: 2.164\n",
            "Validation loss after 25568 examples: 2.138\n",
            "Loss after 26368 examples: 2.291\n",
            "Validation loss after 26368 examples: 2.113\n",
            "Loss after 27168 examples: 2.158\n",
            "Validation loss after 27168 examples: 2.264\n",
            "Loss after 27968 examples: 2.295\n",
            "Validation loss after 27968 examples: 2.142\n",
            "Loss after 28768 examples: 2.082\n",
            "Validation loss after 28768 examples: 2.162\n",
            "Loss after 29568 examples: 2.306\n",
            "Validation loss after 29568 examples: 2.169\n",
            "Loss after 30368 examples: 2.169\n",
            "Validation loss after 30368 examples: 2.149\n",
            "Loss after 31168 examples: 2.105\n",
            "Validation loss after 31168 examples: 2.353\n",
            "Loss after 31968 examples: 2.123\n",
            "Validation loss after 31968 examples: 2.250\n",
            "Loss after 32768 examples: 2.134\n",
            "Validation loss after 32768 examples: 2.167\n",
            "Loss after 33568 examples: 1.929\n",
            "Validation loss after 33568 examples: 2.113\n",
            "Loss after 34368 examples: 2.231\n",
            "Validation loss after 34368 examples: 1.949\n",
            "Loss after 35168 examples: 2.162\n",
            "Validation loss after 35168 examples: 1.979\n",
            "Loss after 35968 examples: 2.186\n",
            "Validation loss after 35968 examples: 2.115\n",
            "Loss after 36768 examples: 2.011\n",
            "Validation loss after 36768 examples: 2.152\n",
            "Loss after 37568 examples: 1.800\n",
            "Validation loss after 37568 examples: 2.017\n",
            "Loss after 38368 examples: 2.111\n",
            "Validation loss after 38368 examples: 2.145\n",
            "Loss after 39168 examples: 2.194\n",
            "Validation loss after 39168 examples: 1.826\n",
            "Loss after 39968 examples: 2.140\n",
            "Validation loss after 39968 examples: 1.991\n",
            "Loss after 40768 examples: 2.076\n",
            "Validation loss after 40768 examples: 2.051\n",
            "Loss after 41568 examples: 2.083\n",
            "Validation loss after 41568 examples: 2.120\n",
            "Loss after 42368 examples: 2.082\n",
            "Validation loss after 42368 examples: 2.006\n",
            "Loss after 43168 examples: 1.881\n",
            "Validation loss after 43168 examples: 1.869\n",
            "Loss after 43968 examples: 1.974\n",
            "Validation loss after 43968 examples: 1.778\n",
            "Loss after 44768 examples: 2.129\n",
            "Validation loss after 44768 examples: 2.087\n",
            "Loss after 45568 examples: 1.939\n",
            "Validation loss after 45568 examples: 2.086\n",
            "Loss after 46368 examples: 1.770\n",
            "Validation loss after 46368 examples: 1.866\n",
            "Loss after 47168 examples: 1.666\n",
            "Validation loss after 47168 examples: 1.846\n",
            "Loss after 47968 examples: 2.012\n",
            "Validation loss after 47968 examples: 2.048\n",
            "Loss after 48768 examples: 2.099\n",
            "Validation loss after 48768 examples: 1.654\n",
            "Loss after 49568 examples: 1.758\n",
            "Validation loss after 49568 examples: 1.652\n",
            "Accuracy of the model on the 1886 test images: 33.13891834570519%\n",
            "Accuracy of the model on the 50000 training images: 33.698%\n",
            "Loss after 50352 examples: 2.354\n",
            "Validation loss after 50352 examples: 2.400\n",
            "Loss after 51152 examples: 2.471\n",
            "Validation loss after 51152 examples: 2.324\n",
            "Loss after 51952 examples: 2.329\n",
            "Validation loss after 51952 examples: 2.258\n",
            "Loss after 52752 examples: 2.351\n",
            "Validation loss after 52752 examples: 2.310\n",
            "Loss after 53552 examples: 2.357\n",
            "Validation loss after 53552 examples: 2.270\n",
            "Loss after 54352 examples: 2.373\n",
            "Validation loss after 54352 examples: 2.291\n",
            "Loss after 55152 examples: 2.339\n",
            "Validation loss after 55152 examples: 2.216\n",
            "Loss after 55952 examples: 2.217\n",
            "Validation loss after 55952 examples: 2.403\n",
            "Loss after 56752 examples: 2.383\n",
            "Validation loss after 56752 examples: 2.391\n",
            "Loss after 57552 examples: 2.229\n",
            "Validation loss after 57552 examples: 2.337\n",
            "Loss after 58352 examples: 2.249\n",
            "Validation loss after 58352 examples: 1.913\n",
            "Loss after 59152 examples: 2.153\n",
            "Validation loss after 59152 examples: 2.250\n",
            "Loss after 59952 examples: 1.904\n",
            "Validation loss after 59952 examples: 2.233\n",
            "Loss after 60752 examples: 2.209\n",
            "Validation loss after 60752 examples: 2.348\n",
            "Loss after 61552 examples: 2.471\n",
            "Validation loss after 61552 examples: 2.145\n",
            "Loss after 62352 examples: 2.214\n",
            "Validation loss after 62352 examples: 2.053\n",
            "Loss after 63152 examples: 2.164\n",
            "Validation loss after 63152 examples: 1.942\n",
            "Loss after 63952 examples: 2.357\n",
            "Validation loss after 63952 examples: 2.294\n",
            "Loss after 64752 examples: 2.250\n",
            "Validation loss after 64752 examples: 2.371\n",
            "Loss after 65552 examples: 1.891\n",
            "Validation loss after 65552 examples: 1.960\n",
            "Loss after 66352 examples: 2.316\n",
            "Validation loss after 66352 examples: 2.056\n",
            "Loss after 67152 examples: 2.143\n",
            "Validation loss after 67152 examples: 1.812\n",
            "Loss after 67952 examples: 1.984\n",
            "Validation loss after 67952 examples: 1.876\n",
            "Loss after 68752 examples: 2.167\n",
            "Validation loss after 68752 examples: 1.891\n",
            "Loss after 69552 examples: 1.887\n",
            "Validation loss after 69552 examples: 2.005\n",
            "Loss after 70352 examples: 1.888\n",
            "Validation loss after 70352 examples: 2.095\n",
            "Loss after 71152 examples: 2.013\n",
            "Validation loss after 71152 examples: 2.128\n",
            "Loss after 71952 examples: 1.862\n",
            "Validation loss after 71952 examples: 1.829\n",
            "Loss after 72752 examples: 1.861\n",
            "Validation loss after 72752 examples: 1.968\n",
            "Loss after 73552 examples: 1.771\n",
            "Validation loss after 73552 examples: 2.202\n",
            "Loss after 74352 examples: 1.800\n",
            "Validation loss after 74352 examples: 1.839\n",
            "Loss after 75152 examples: 1.921\n",
            "Validation loss after 75152 examples: 1.754\n",
            "Loss after 75952 examples: 2.127\n",
            "Validation loss after 75952 examples: 1.748\n",
            "Loss after 76752 examples: 1.922\n",
            "Validation loss after 76752 examples: 1.622\n",
            "Loss after 77552 examples: 2.202\n",
            "Validation loss after 77552 examples: 2.017\n",
            "Loss after 78352 examples: 1.911\n",
            "Validation loss after 78352 examples: 2.063\n",
            "Loss after 79152 examples: 1.973\n",
            "Validation loss after 79152 examples: 1.888\n",
            "Loss after 79952 examples: 1.863\n",
            "Validation loss after 79952 examples: 2.159\n",
            "Loss after 80752 examples: 2.189\n",
            "Validation loss after 80752 examples: 2.096\n",
            "Loss after 81552 examples: 1.462\n",
            "Validation loss after 81552 examples: 2.028\n",
            "Loss after 82352 examples: 1.940\n",
            "Validation loss after 82352 examples: 1.856\n",
            "Loss after 83152 examples: 1.871\n",
            "Validation loss after 83152 examples: 1.738\n",
            "Loss after 83952 examples: 1.585\n",
            "Validation loss after 83952 examples: 1.829\n",
            "Loss after 84752 examples: 1.951\n",
            "Validation loss after 84752 examples: 1.690\n",
            "Loss after 85552 examples: 1.740\n",
            "Validation loss after 85552 examples: 1.818\n",
            "Loss after 86352 examples: 1.924\n",
            "Validation loss after 86352 examples: 1.781\n",
            "Loss after 87152 examples: 1.906\n",
            "Validation loss after 87152 examples: 1.793\n",
            "Loss after 87952 examples: 2.095\n",
            "Validation loss after 87952 examples: 1.651\n",
            "Loss after 88752 examples: 1.847\n",
            "Validation loss after 88752 examples: 1.766\n",
            "Loss after 89552 examples: 2.073\n",
            "Validation loss after 89552 examples: 1.990\n",
            "Loss after 90352 examples: 2.014\n",
            "Validation loss after 90352 examples: 1.738\n",
            "Loss after 91152 examples: 1.878\n",
            "Validation loss after 91152 examples: 1.846\n",
            "Loss after 91952 examples: 1.937\n",
            "Validation loss after 91952 examples: 1.920\n",
            "Loss after 92752 examples: 1.630\n",
            "Validation loss after 92752 examples: 1.836\n",
            "Loss after 93552 examples: 1.478\n",
            "Validation loss after 93552 examples: 1.828\n",
            "Loss after 94352 examples: 1.746\n",
            "Validation loss after 94352 examples: 2.162\n",
            "Loss after 95152 examples: 1.808\n",
            "Validation loss after 95152 examples: 1.651\n",
            "Loss after 95952 examples: 1.791\n",
            "Validation loss after 95952 examples: 1.623\n",
            "Loss after 96752 examples: 1.716\n",
            "Validation loss after 96752 examples: 1.675\n",
            "Loss after 97552 examples: 2.160\n",
            "Validation loss after 97552 examples: 1.687\n",
            "Loss after 98352 examples: 2.131\n",
            "Validation loss after 98352 examples: 1.873\n",
            "Loss after 99152 examples: 1.671\n",
            "Validation loss after 99152 examples: 1.612\n",
            "Loss after 99952 examples: 1.659\n",
            "Validation loss after 99952 examples: 1.915\n",
            "Accuracy of the model on the 1886 test images: 43.05408271474019%\n",
            "Accuracy of the model on the 50000 training images: 41.288%\n",
            "Loss after 100736 examples: 1.555\n",
            "Validation loss after 100736 examples: 2.034\n",
            "Loss after 101536 examples: 1.611\n",
            "Validation loss after 101536 examples: 1.369\n",
            "Loss after 102336 examples: 1.994\n",
            "Validation loss after 102336 examples: 1.658\n",
            "Loss after 103136 examples: 1.581\n",
            "Validation loss after 103136 examples: 1.831\n",
            "Loss after 103936 examples: 2.119\n",
            "Validation loss after 103936 examples: 1.933\n",
            "Loss after 104736 examples: 1.641\n",
            "Validation loss after 104736 examples: 1.803\n",
            "Loss after 105536 examples: 1.334\n",
            "Validation loss after 105536 examples: 2.214\n",
            "Loss after 106336 examples: 1.712\n",
            "Validation loss after 106336 examples: 1.440\n",
            "Loss after 107136 examples: 2.069\n",
            "Validation loss after 107136 examples: 1.658\n",
            "Loss after 107936 examples: 1.544\n",
            "Validation loss after 107936 examples: 1.725\n",
            "Loss after 108736 examples: 1.858\n",
            "Validation loss after 108736 examples: 1.870\n",
            "Loss after 109536 examples: 1.424\n",
            "Validation loss after 109536 examples: 1.699\n",
            "Loss after 110336 examples: 1.594\n",
            "Validation loss after 110336 examples: 1.511\n",
            "Loss after 111136 examples: 1.693\n",
            "Validation loss after 111136 examples: 1.726\n",
            "Loss after 111936 examples: 1.332\n",
            "Validation loss after 111936 examples: 1.940\n",
            "Loss after 112736 examples: 1.649\n",
            "Validation loss after 112736 examples: 1.417\n",
            "Loss after 113536 examples: 2.012\n",
            "Validation loss after 113536 examples: 1.602\n",
            "Loss after 114336 examples: 1.856\n",
            "Validation loss after 114336 examples: 1.583\n",
            "Loss after 115136 examples: 1.539\n",
            "Validation loss after 115136 examples: 1.883\n",
            "Loss after 115936 examples: 2.007\n",
            "Validation loss after 115936 examples: 1.782\n",
            "Loss after 116736 examples: 1.571\n",
            "Validation loss after 116736 examples: 1.721\n",
            "Loss after 117536 examples: 1.642\n",
            "Validation loss after 117536 examples: 1.558\n",
            "Loss after 118336 examples: 1.433\n",
            "Validation loss after 118336 examples: 1.688\n",
            "Loss after 119136 examples: 1.554\n",
            "Validation loss after 119136 examples: 1.855\n",
            "Loss after 119936 examples: 1.613\n",
            "Validation loss after 119936 examples: 1.319\n",
            "Loss after 120736 examples: 1.594\n",
            "Validation loss after 120736 examples: 1.171\n",
            "Loss after 121536 examples: 1.913\n",
            "Validation loss after 121536 examples: 2.143\n",
            "Loss after 122336 examples: 1.541\n",
            "Validation loss after 122336 examples: 1.454\n",
            "Loss after 123136 examples: 1.951\n",
            "Validation loss after 123136 examples: 1.729\n",
            "Loss after 123936 examples: 1.877\n",
            "Validation loss after 123936 examples: 1.538\n",
            "Loss after 124736 examples: 1.372\n",
            "Validation loss after 124736 examples: 1.649\n",
            "Loss after 125536 examples: 1.894\n",
            "Validation loss after 125536 examples: 1.726\n",
            "Loss after 126336 examples: 1.649\n",
            "Validation loss after 126336 examples: 1.721\n",
            "Loss after 127136 examples: 1.482\n",
            "Validation loss after 127136 examples: 1.368\n",
            "Loss after 127936 examples: 1.675\n",
            "Validation loss after 127936 examples: 1.610\n",
            "Loss after 128736 examples: 1.607\n",
            "Validation loss after 128736 examples: 1.528\n",
            "Loss after 129536 examples: 1.559\n",
            "Validation loss after 129536 examples: 1.631\n",
            "Loss after 130336 examples: 1.710\n",
            "Validation loss after 130336 examples: 1.435\n",
            "Loss after 131136 examples: 1.502\n",
            "Validation loss after 131136 examples: 1.485\n",
            "Loss after 131936 examples: 1.343\n",
            "Validation loss after 131936 examples: 1.500\n",
            "Loss after 132736 examples: 1.813\n",
            "Validation loss after 132736 examples: 1.512\n",
            "Loss after 133536 examples: 1.695\n",
            "Validation loss after 133536 examples: 1.753\n",
            "Loss after 134336 examples: 1.685\n",
            "Validation loss after 134336 examples: 1.530\n",
            "Loss after 135136 examples: 1.330\n",
            "Validation loss after 135136 examples: 1.597\n",
            "Loss after 135936 examples: 1.610\n",
            "Validation loss after 135936 examples: 1.460\n",
            "Loss after 136736 examples: 1.620\n",
            "Validation loss after 136736 examples: 2.002\n",
            "Loss after 137536 examples: 1.720\n",
            "Validation loss after 137536 examples: 1.346\n",
            "Loss after 138336 examples: 1.508\n",
            "Validation loss after 138336 examples: 1.892\n",
            "Loss after 139136 examples: 1.575\n",
            "Validation loss after 139136 examples: 1.679\n",
            "Loss after 139936 examples: 1.753\n",
            "Validation loss after 139936 examples: 1.344\n",
            "Loss after 140736 examples: 1.233\n",
            "Validation loss after 140736 examples: 1.593\n",
            "Loss after 141536 examples: 1.491\n",
            "Validation loss after 141536 examples: 1.630\n",
            "Loss after 142336 examples: 1.559\n",
            "Validation loss after 142336 examples: 1.595\n",
            "Loss after 143136 examples: 1.874\n",
            "Validation loss after 143136 examples: 1.396\n",
            "Loss after 143936 examples: 1.510\n",
            "Validation loss after 143936 examples: 1.757\n",
            "Loss after 144736 examples: 1.394\n",
            "Validation loss after 144736 examples: 1.303\n",
            "Loss after 145536 examples: 1.436\n",
            "Validation loss after 145536 examples: 2.126\n",
            "Loss after 146336 examples: 1.694\n",
            "Validation loss after 146336 examples: 1.436\n",
            "Loss after 147136 examples: 1.480\n",
            "Validation loss after 147136 examples: 1.804\n",
            "Loss after 147936 examples: 1.475\n",
            "Validation loss after 147936 examples: 1.669\n",
            "Loss after 148736 examples: 1.149\n",
            "Validation loss after 148736 examples: 1.294\n",
            "Loss after 149536 examples: 1.479\n",
            "Validation loss after 149536 examples: 1.528\n",
            "Accuracy of the model on the 1886 test images: 39.554612937433724%\n",
            "Accuracy of the model on the 50000 training images: 39.384%\n",
            "Loss after 150320 examples: 1.412\n",
            "Validation loss after 150320 examples: 1.513\n",
            "Loss after 151120 examples: 1.250\n",
            "Validation loss after 151120 examples: 1.566\n",
            "Loss after 151920 examples: 1.686\n",
            "Validation loss after 151920 examples: 1.315\n",
            "Loss after 152720 examples: 1.400\n",
            "Validation loss after 152720 examples: 1.369\n",
            "Loss after 153520 examples: 1.608\n",
            "Validation loss after 153520 examples: 1.499\n",
            "Loss after 154320 examples: 1.611\n",
            "Validation loss after 154320 examples: 1.627\n",
            "Loss after 155120 examples: 1.138\n",
            "Validation loss after 155120 examples: 1.346\n",
            "Loss after 155920 examples: 1.340\n",
            "Validation loss after 155920 examples: 1.555\n",
            "Loss after 156720 examples: 1.517\n",
            "Validation loss after 156720 examples: 1.369\n",
            "Loss after 157520 examples: 1.741\n",
            "Validation loss after 157520 examples: 1.369\n",
            "Loss after 158320 examples: 1.372\n",
            "Validation loss after 158320 examples: 1.488\n",
            "Loss after 159120 examples: 1.541\n",
            "Validation loss after 159120 examples: 1.187\n",
            "Loss after 159920 examples: 1.589\n",
            "Validation loss after 159920 examples: 1.789\n",
            "Loss after 160720 examples: 1.447\n",
            "Validation loss after 160720 examples: 1.779\n",
            "Loss after 161520 examples: 1.187\n",
            "Validation loss after 161520 examples: 1.499\n",
            "Loss after 162320 examples: 1.423\n",
            "Validation loss after 162320 examples: 1.549\n",
            "Loss after 163120 examples: 1.511\n",
            "Validation loss after 163120 examples: 1.189\n",
            "Loss after 163920 examples: 1.290\n",
            "Validation loss after 163920 examples: 1.857\n",
            "Loss after 164720 examples: 1.313\n",
            "Validation loss after 164720 examples: 1.567\n",
            "Loss after 165520 examples: 1.333\n",
            "Validation loss after 165520 examples: 1.308\n",
            "Loss after 166320 examples: 1.893\n",
            "Validation loss after 166320 examples: 2.042\n",
            "Loss after 167120 examples: 1.314\n",
            "Validation loss after 167120 examples: 1.189\n",
            "Loss after 167920 examples: 1.152\n",
            "Validation loss after 167920 examples: 1.505\n",
            "Loss after 168720 examples: 1.582\n",
            "Validation loss after 168720 examples: 1.342\n",
            "Loss after 169520 examples: 1.248\n",
            "Validation loss after 169520 examples: 1.343\n",
            "Loss after 170320 examples: 1.472\n",
            "Validation loss after 170320 examples: 1.087\n",
            "Loss after 171120 examples: 1.229\n",
            "Validation loss after 171120 examples: 1.412\n",
            "Loss after 171920 examples: 1.453\n",
            "Validation loss after 171920 examples: 1.058\n",
            "Loss after 172720 examples: 1.446\n",
            "Validation loss after 172720 examples: 1.408\n",
            "Loss after 173520 examples: 1.443\n",
            "Validation loss after 173520 examples: 1.534\n",
            "Loss after 174320 examples: 1.411\n",
            "Validation loss after 174320 examples: 1.576\n",
            "Loss after 175120 examples: 1.368\n",
            "Validation loss after 175120 examples: 1.506\n",
            "Loss after 175920 examples: 1.348\n",
            "Validation loss after 175920 examples: 1.376\n",
            "Loss after 176720 examples: 1.400\n",
            "Validation loss after 176720 examples: 1.210\n",
            "Loss after 177520 examples: 1.888\n",
            "Validation loss after 177520 examples: 1.177\n",
            "Loss after 178320 examples: 1.025\n",
            "Validation loss after 178320 examples: 1.670\n",
            "Loss after 179120 examples: 1.882\n",
            "Validation loss after 179120 examples: 1.279\n",
            "Loss after 179920 examples: 1.555\n",
            "Validation loss after 179920 examples: 1.397\n",
            "Loss after 180720 examples: 1.253\n",
            "Validation loss after 180720 examples: 1.202\n",
            "Loss after 181520 examples: 1.234\n",
            "Validation loss after 181520 examples: 1.699\n",
            "Loss after 182320 examples: 1.524\n",
            "Validation loss after 182320 examples: 1.378\n",
            "Loss after 183120 examples: 1.458\n",
            "Validation loss after 183120 examples: 1.242\n",
            "Loss after 183920 examples: 1.211\n",
            "Validation loss after 183920 examples: 0.982\n",
            "Loss after 184720 examples: 1.636\n",
            "Validation loss after 184720 examples: 1.366\n",
            "Loss after 185520 examples: 1.177\n",
            "Validation loss after 185520 examples: 1.841\n",
            "Loss after 186320 examples: 1.484\n",
            "Validation loss after 186320 examples: 1.628\n",
            "Loss after 187120 examples: 1.399\n",
            "Validation loss after 187120 examples: 1.691\n",
            "Loss after 187920 examples: 1.660\n",
            "Validation loss after 187920 examples: 1.354\n",
            "Loss after 188720 examples: 1.638\n",
            "Validation loss after 188720 examples: 1.496\n",
            "Loss after 189520 examples: 1.438\n",
            "Validation loss after 189520 examples: 1.224\n",
            "Loss after 190320 examples: 1.732\n",
            "Validation loss after 190320 examples: 1.088\n",
            "Loss after 191120 examples: 1.521\n",
            "Validation loss after 191120 examples: 1.189\n",
            "Loss after 191920 examples: 1.381\n",
            "Validation loss after 191920 examples: 1.239\n",
            "Loss after 192720 examples: 1.347\n",
            "Validation loss after 192720 examples: 1.200\n",
            "Loss after 193520 examples: 1.513\n",
            "Validation loss after 193520 examples: 1.171\n",
            "Loss after 194320 examples: 1.570\n",
            "Validation loss after 194320 examples: 1.082\n",
            "Loss after 195120 examples: 1.204\n",
            "Validation loss after 195120 examples: 1.383\n",
            "Loss after 195920 examples: 1.254\n",
            "Validation loss after 195920 examples: 1.288\n",
            "Loss after 196720 examples: 1.556\n",
            "Validation loss after 196720 examples: 1.496\n",
            "Loss after 197520 examples: 1.194\n",
            "Validation loss after 197520 examples: 1.210\n",
            "Loss after 198320 examples: 1.187\n",
            "Validation loss after 198320 examples: 1.369\n",
            "Loss after 199120 examples: 1.102\n",
            "Validation loss after 199120 examples: 1.027\n",
            "Loss after 199920 examples: 1.300\n",
            "Validation loss after 199920 examples: 1.019\n",
            "Accuracy of the model on the 1886 test images: 52.86320254506893%\n",
            "Accuracy of the model on the 50000 training images: 52.606%\n",
            "Loss after 200704 examples: 1.196\n",
            "Validation loss after 200704 examples: 1.079\n",
            "Loss after 201504 examples: 1.523\n",
            "Validation loss after 201504 examples: 1.490\n",
            "Loss after 202304 examples: 1.233\n",
            "Validation loss after 202304 examples: 1.704\n",
            "Loss after 203104 examples: 1.139\n",
            "Validation loss after 203104 examples: 1.283\n",
            "Loss after 203904 examples: 1.307\n",
            "Validation loss after 203904 examples: 1.320\n",
            "Loss after 204704 examples: 1.123\n",
            "Validation loss after 204704 examples: 1.180\n",
            "Loss after 205504 examples: 1.295\n",
            "Validation loss after 205504 examples: 1.616\n",
            "Loss after 206304 examples: 1.091\n",
            "Validation loss after 206304 examples: 1.135\n",
            "Loss after 207104 examples: 1.018\n",
            "Validation loss after 207104 examples: 1.606\n",
            "Loss after 207904 examples: 1.132\n",
            "Validation loss after 207904 examples: 1.400\n",
            "Loss after 208704 examples: 1.143\n",
            "Validation loss after 208704 examples: 1.334\n",
            "Loss after 209504 examples: 1.349\n",
            "Validation loss after 209504 examples: 1.585\n",
            "Loss after 210304 examples: 1.168\n",
            "Validation loss after 210304 examples: 1.078\n",
            "Loss after 211104 examples: 1.339\n",
            "Validation loss after 211104 examples: 1.344\n",
            "Loss after 211904 examples: 2.177\n",
            "Validation loss after 211904 examples: 2.046\n",
            "Loss after 212704 examples: 1.002\n",
            "Validation loss after 212704 examples: 1.721\n",
            "Loss after 213504 examples: 0.962\n",
            "Validation loss after 213504 examples: 1.510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcVwRwUl0chy"
      },
      "source": [
        "#### Soumission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmiiJmjZvUrA"
      },
      "source": [
        "def make_submission(x_test, network):\n",
        "    header = [\"Id\", \"class\"]\n",
        "    id_value = 0\n",
        "\n",
        "    classifications =[]\n",
        "    for image in x_test:\n",
        "        image = torch.reshape(image[0], (1,1,96, 96))\n",
        "        classifications.append([id_value, network(image.float()).argmax(dim=1)[0].item()])\n",
        "        id_value += 1\n",
        "\n",
        "    y = PATH+\"./submission_3dec.csv\"\n",
        "    with open(y, 'w', newline='') as work:\n",
        "        z = csv.writer(work)\n",
        "        z.writerow(header)\n",
        "        z.writerows(classifications)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQhJ6tEPvMMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e395b0f-975a-4da2-debd-e5920a89868c"
      },
      "source": [
        "#loading data\n",
        "x_test_scalar =  np.array(pkl.load(open(PATH+'x_test.pkl', 'rb')))\n",
        "x_test = x_test_scalar\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=float)[:, None, :, :].to(device)\n",
        "y_test = torch.tensor(np.array([1 for y in x_test]), dtype=float)\n",
        "\n",
        "trans = transforms.Compose(\n",
        "    [transforms.Normalize((0.5), (0.5))]\n",
        ")\n",
        "#Transformation\n",
        "test_dataset = CustomTensorDataset(tensors=(x_test, y_test), transform=trans)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1,\n",
        "                                          shuffle=False)\n",
        "#Insert model\n",
        "network = AlexNetv2().to(device)\n",
        "network.load_state_dict(torch.load('./model.pth'))\n",
        "network.eval()\n",
        "\n",
        "make_submission(test_dataloader, network)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNetv2\n"
          ]
        }
      ]
    }
  ]
}
