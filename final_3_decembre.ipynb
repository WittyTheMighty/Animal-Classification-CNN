{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "final_3_decembre.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5ecdebda2b9e4633bb4d367ab36d62b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_744330ae08ab4ac887c3b79efd339a53",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1fdf06341a93493fb354bb21c4b16d6d",
              "IPY_MODEL_95f08d39b4c44ab6be2d20482c4600f1",
              "IPY_MODEL_2abeac70f21b4daeba99ea5362f40dea"
            ]
          }
        },
        "744330ae08ab4ac887c3b79efd339a53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1fdf06341a93493fb354bb21c4b16d6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_79d18efcd4f84255b2f89f2127e95240",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 60%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b46bbc12d81c469e967ea0f9656adad1"
          }
        },
        "95f08d39b4c44ab6be2d20482c4600f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b1de955c583841779fdd926a7f4dceeb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 6,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3025589946844141a684db2189626c8d"
          }
        },
        "2abeac70f21b4daeba99ea5362f40dea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f8eee69ad35742a488571b934ee3e063",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6/10 [15:46&lt;10:30, 157.56s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_96a9fd56da3e48af8b0da50c1a99c8a8"
          }
        },
        "79d18efcd4f84255b2f89f2127e95240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b46bbc12d81c469e967ea0f9656adad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1de955c583841779fdd926a7f4dceeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3025589946844141a684db2189626c8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8eee69ad35742a488571b934ee3e063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "96a9fd56da3e48af8b0da50c1a99c8a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WittyTheMighty/ML-kaggle/blob/main/final_3_decembre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZiJViSwLeJ8"
      },
      "source": [
        "# Final Kaggle competition ML"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cffyOyN3hsU",
        "outputId": "676ff1e3-570d-4dd6-9ad6-fa24849c6d25"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meAL21kDHHbV"
      },
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s9PMiG1H9mX"
      },
      "source": [
        "This is how we can identify the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq9PVvTaHc_y"
      },
      "source": [
        "#Exemple d'utilisation \n",
        "\n",
        "# import wandb\n",
        "# # At the top of your training script, start a new run\n",
        "# wandb.init(project=\"test-project\", entity=\"xabjuwplb\")\n",
        "# # Capture a dictionary of hyperparameters with config\n",
        "# wandb.config = {\n",
        "#   \"learning_rate\": 0.001,\n",
        "#   \"epochs\": 100,\n",
        "#   \"batch_size\": 128\n",
        "# }\n",
        "\n",
        "# # Log metrics inside your training loop to visualize model performance\n",
        "# wandb.log({\"loss\": loss})\n",
        "\n",
        "# # Optional\n",
        "# wandb.watch(model)\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PZkAi5kFRGZ"
      },
      "source": [
        "###Import: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50NROwOnNqC6"
      },
      "source": [
        "# imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch as torch\n",
        "import torchvision as  tv\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "from matplotlib import cm\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "import csv\n",
        "import scipy as sp\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.utils as vutils\n",
        "device = torch.device('cuda')\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.notebook import tqdm\n",
        "import wandb\n",
        "from sklearn.metrics import f1_score\n",
        "# Ensure deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
        "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
        "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
        "\n",
        "# Device configuration\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y5rJHrRFVOD"
      },
      "source": [
        "######Weigth and biais imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiNOnDapFcsH"
      },
      "source": [
        "To track our model we use weights and biaises a plateform that allow us to track the data in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNFNbttcFaI3"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uAeECFFFUxy"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4JPyre6L8EO"
      },
      "source": [
        "#PATH = './drive/MyDrive/ML-Kaggle/'\n",
        "PATH = \"/content/drive/MyDrive/ML-Kaggle/\"\n",
        "# PATH = '../data/'\n",
        "\n",
        "#Load\n",
        "x_data = np.array(pkl.load(open(PATH+'x_train.pkl', 'rb')))\n",
        "y_data = np.array(pkl.load(open(PATH+'y_train.pkl', 'rb')))\n",
        "\n",
        "x_test_submission =  np.array(pkl.load(open(PATH+'x_test.pkl', 'rb')))\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1c10XknRDX9"
      },
      "source": [
        "x_train = x_data[:10000]\n",
        "y_train = y_data[:10000]\n",
        "\n",
        "#Before submit we test the model on this subsection of the dataset without augmented data\n",
        "x_valid = x_data[10001:]\n",
        "y_valid = y_data[10001:]\n",
        "\n",
        "assert len(x_train) == len(y_train)\n",
        "assert len(x_valid) == len(y_valid)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liw-MzMqO8d7"
      },
      "source": [
        "labels = np.unique(y_train)\n",
        "labels = dict(zip(labels, range(len(labels))))"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvLBaLGxFyYP"
      },
      "source": [
        "labels = np.unique(y_valid)\n",
        "labels=dict(zip(labels, range(len(labels))))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEBNTESWXEJo"
      },
      "source": [
        "---\n",
        "\n",
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR-Bj3-NSU7G"
      },
      "source": [
        "####Data augmentation from strach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4i4BXE7PHFR"
      },
      "source": [
        "#First preprocessing step \n",
        "\n",
        "#Utilitary function for geometric preprocessing\n",
        "# def flip_verticaly(X):\n",
        "#     flipped_image = copy.deepcopy(rotate90(X))\n",
        "#     for idx,channel in enumerate(flipped_image):\n",
        "#         flipped_image[idx]= np.fliplr(channel)\n",
        "\n",
        "#     return rotate(flipped_image, 270)\n",
        "\n",
        "\n",
        "def flip_horizontaly(X):\n",
        "    flipped_image = copy.deepcopy(X)\n",
        "    flipped_image= np.fliplr(flipped_image)\n",
        "\n",
        "    return flipped_image\n",
        "\n",
        "def rotate90(X):\n",
        "    rot90_img = copy.deepcopy(X)\n",
        "    rot90_img= np.rot90(rot90_img)\n",
        "    return rot90_img\n",
        "\n",
        "def rotate(X,degree):\n",
        "    rot_img = copy.deepcopy(X)\n",
        "    rot_img= sp.ndimage.rotate(rot_img, degree, reshape=False)\n",
        "    return rot_img\n",
        "\n",
        "#An adaptation from:\n",
        "# https://stackoverflow.com/questions/37119071/scipy-rotate-and-zoom-an-image-without-changing-its-dimensions\n",
        "def clipped_zoom(img, zoom_factor):\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    # For multichannel images we don't want to apply the zoom factor to the RGB\n",
        "    # dimension, so instead we create a tuple of zoom factors, one per array\n",
        "    # dimension, with 1's for any trailing dimensions after the width and height.\n",
        "    zoom_tuple = (zoom_factor,) * 2 + (1,) * (img.ndim - 2)\n",
        "\n",
        "    # Zooming out\n",
        "    if zoom_factor < 1:\n",
        "\n",
        "        # Bounding box of the zoomed-out image within the output array\n",
        "        zh = int(np.round(h * zoom_factor))\n",
        "        zw = int(np.round(w * zoom_factor))\n",
        "        top = (h - zh) // 2\n",
        "        left = (w - zw) // 2\n",
        "\n",
        "        # Zero-padding\n",
        "        out = np.zeros_like(img)\n",
        "        out[top:top+zh, left:left+zw] = sp.ndimage.zoom(img, zoom_tuple)\n",
        "\n",
        "    # Zooming in\n",
        "    elif zoom_factor > 1:\n",
        "\n",
        "        # Bounding box of the zoomed-in region within the input array\n",
        "        zh = int(np.round(h / zoom_factor))\n",
        "        zw = int(np.round(w / zoom_factor))\n",
        "        top = (h - zh) // 2\n",
        "        left = (w - zw) // 2\n",
        "\n",
        "        out = sp.ndimage.zoom(img[top:top+zh, left:left+zw], zoom_tuple)\n",
        "\n",
        "        # `out` might still be slightly larger than `img` due to rounding, so\n",
        "        # trim off any extra pixels at the edges\n",
        "        trim_top = ((out.shape[0] - h) // 2)\n",
        "        trim_left = ((out.shape[1] - w) // 2)\n",
        "        out = out[trim_top:trim_top+h, trim_left:trim_left+w]\n",
        "\n",
        "    # If zoom_factor == 1, just return the input array\n",
        "    else:\n",
        "        out = img\n",
        "    return out\n",
        "\n",
        "def blur(X,blur_factor):\n",
        "    blur_img = copy.deepcopy(X)\n",
        "    blur_img= sp.ndimage.gaussian_filter(blur_img, sigma=blur_factor)\n",
        "    return blur_img\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeN0vhmWIM6U"
      },
      "source": [
        "zoom_factors = [1.1, 1.3, 1.5]\n",
        "x_train_zoomed_vertical = np.array([clipped_zoom(x,np.random.choice(zoom_factors)) for x in x_train])\n",
        "x_train_zoomed_horizontal = np.array([rotate(clipped_zoom(rotate90(x),np.random.choice(zoom_factors)),270) for x in x_train])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3BSJ-sUOcVP"
      },
      "source": [
        "x_train_rotated = np.array([rotate(x,round(np.random.random()*360)) for x in x_train])"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9NSG6ZuO0a5"
      },
      "source": [
        "x_train_flipped_horizontally = np.array([flip_horizontaly(x) for x in x_train])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOvc4L1-Q4MH"
      },
      "source": [
        ""
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E92H1A0LQ-yX"
      },
      "source": [
        "augmented_x_train = np.concatenate([x_train, x_train_zoomed_horizontal, x_train_zoomed_vertical, x_train_rotated, x_train_flipped_horizontally], axis=0)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQcpU2gNROs1"
      },
      "source": [
        "augmented_y_train = np.concatenate([y_train]*5)\n",
        "augmented_y_train.shape\n",
        "\n",
        "X = np.array([[1., 0.], [2., 1.], [0., 0.]])\n",
        "y = np.array([0, 1, 2])\n",
        "\n",
        "X_train, y_train= shuffle(augmented_x_train, augmented_y_train, random_state=0)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNgfEHbmRsyL"
      },
      "source": [
        "# SAVE AUGMENTED TRAINING SET\n",
        "pkl.dump(augmented_x_train,open(PATH+'x_train_aug.pkl', 'wb'))\n",
        "pkl.dump(augmented_y_train,open(PATH+'y_train_aug.pkl', 'wb'))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8su14mjWSMp2"
      },
      "source": [
        "# LOAD AUGMENTED TRAINING SET\n",
        "x_train = pkl.load(open(PATH+'x_train_aug.pkl', 'rb'))\n",
        "y_train = pkl.load(open(PATH+'y_train_aug.pkl', 'rb'))"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB5RSJ6UuE41"
      },
      "source": [
        "###Normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szwqrVXaLJB0"
      },
      "source": [
        "From strach not used yet. We use pytorch normalize function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2UsbeWttv6y"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "def standardize(data):\n",
        "    standardized_data = []\n",
        "    scaler = StandardScaler()\n",
        "    for img in data:\n",
        "        scaler = scaler.fit(img)\n",
        "        standadized_data.append(scaler.transform(img))\n",
        "    return np.array(standardized_data)\n",
        "\n",
        "def normalize(data):\n",
        "    normalized_data = []\n",
        "    normalizer = Normalizer()\n",
        "    for img in data:\n",
        "        normalizer = normalizer.fit(img)\n",
        "        normalized_data.append(normalizer.transform(img))\n",
        "    return np.array(normalized_data)\n",
        "\n",
        "\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-MDKSh4IAXa"
      },
      "source": [
        "\n",
        "# x_train_normalize = normalize(x_train)\n",
        "\n",
        "# #Todo : Normaliser les données de tests\n",
        "\n",
        "# x_test_set = x_train_normalize[:3000]\n",
        "# y_test_set = y_train[:3000]\n",
        "\n",
        "# x_train = x_train_normalize[3000:]\n",
        "# y_train = y_train[3000:]\n",
        "\n",
        "# assert y_train.shape[0] == x_train.shape[0]\n",
        "# assert y_test_set.shape[0] == x_test_set.shape[0]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be_PY1e8XJ2r"
      },
      "source": [
        "---\n",
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfszZAClLPtK"
      },
      "source": [
        "This is all the model we used in our expirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKdxRj-W3iF_"
      },
      "source": [
        "##### Inspired AlexNet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ4o50Gldcm5"
      },
      "source": [
        "\n",
        "class AlexNet(nn.Module): \n",
        "    def __init__(self): \n",
        "        super().__init__()\n",
        "        print(\"AlexNet\")\n",
        "        self.conv1 = nn.Conv2d(1,25,5)\n",
        "        self.batchNorm2d1 = nn.BatchNorm2d(25,momentum=0.95)\n",
        "        # we use the maxpool multiple times, but define it once\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # in_channels = 6 because self.conv1 output 6 channel\n",
        "        self.conv2 = nn.Conv2d(25, 50,5)\n",
        "        self.batchNorm2d2 = nn.BatchNorm2d(50,momentum=0.95)\n",
        "        self.conv3 = nn.Conv2d(50 ,100,3)\n",
        "        self.batchNorm2d3 = nn.BatchNorm2d(100,momentum=0.95)\n",
        "        # 5*5 comes from the dimension of the last convnet layer\n",
        "        self.fc1 = nn.Linear(100*9*9, 480) \n",
        "        self.fc2 = nn.Linear(480, 240)\n",
        "        self.fc3 = nn.Linear(240, 120)\n",
        "        self.fc4 = nn.Linear(120, 11)\n",
        "        \n",
        "    def forward(self, x): \n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.batchNorm2d1(x)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.batchNorm2d2(x)\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.batchNorm2d3(x)\n",
        "        x = x.view(-1, 100*9*9)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)  # no activation on final layer \n",
        "        return x\n",
        "\n",
        "\n",
        "class AlexNetv2(nn.Module): \n",
        "    def __init__(self): \n",
        "        super().__init__()\n",
        "        print(\"AlexNetv2\")\n",
        "        self.convolution = nn.Sequential(\n",
        "            #Layer 1 96 x 96  -> (96 - 7) +1 = 90 \n",
        "            nn.Conv2d(1, 48,7), #in, out, kernel_size, stride,padding\n",
        "            # 90 x 90 : \n",
        "            # (90 -2)/2 +1\n",
        "            nn.MaxPool2d(2,stride=2), #kernel_size\n",
        "            nn.ReLU(),\n",
        "            #Layer 2\n",
        "            # 45x45  \n",
        "            nn.Conv2d(48, 128,5,stride=2), #(45-5)/2 +1 = 21\n",
        "            #41x41 = (21-3)/2 +1\n",
        "            nn.MaxPool2d(3,2),\n",
        "            nn.ReLU(),\n",
        "            # 10x10 (10-5+(2*2)) +1 =10\n",
        "            nn.Conv2d(128, 192, 5,padding=2), \n",
        "            nn.ReLU(),\n",
        "            #10x10\n",
        "            #(10-3+(2*1)) +1 =10\n",
        "            nn.Conv2d(192, 192, 3,padding=1), #(39-3+1)/1 =37\n",
        "            nn.ReLU(),\n",
        "            #10x10\n",
        "            #(10-3+(2*1)) +1 =10\n",
        "            nn.Conv2d(192, 128, 3, 1),# 35\n",
        "            nn.ReLU(),\n",
        "            #(10-3+(2*1)) +1 =10\n",
        "            # 10 x 10\n",
        "            nn.Conv2d(128, 128, 3, 1), \n",
        "            # (10-2)/2 +1\n",
        "            nn.MaxPool2d(2,stride=2),\n",
        "            #5x5\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            #3200 = 5 * 5 *128\n",
        "            nn.Linear(128*3*3, 4096),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Linear(4096, 11),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x): \n",
        "        x = self.convolution(x)\n",
        "        x = x.view(-1,128*3*3)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTUzHLoq3nRt"
      },
      "source": [
        "#### Inspired ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRRuZnpjLIqh"
      },
      "source": [
        "##Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EqagABGLDZ0"
      },
      "source": [
        "For our pipeline we used the pipeline framework weigths and biaises for easy and fast data parameter tracking. All the result of our experiments will be store on the cloud. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksZme17IPrHY"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# device = torch.device('cuda')\n",
        "#device = torch.device(\"cpu\")\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "class CustomTensorDataset(Dataset):\n",
        "    \"\"\"TensorDataset with support of transforms.\"\"\"\n",
        "    def __init__(self, tensors, transform=None):\n",
        "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
        "        self.tensors = tensors\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.tensors[0][index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        y = self.tensors[1][index]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tensors[0].size(0)\n",
        "    \n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOqyeUJYKa4T"
      },
      "source": [
        "#####Custom TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2bImeQ2veNv"
      },
      "source": [
        "#Creating dataloader object\n",
        "trans = transforms.Compose(\n",
        "    [transforms.Normalize((0.5), (0.5))]\n",
        ")\n",
        "\n",
        "\n",
        "#Training set\n",
        "x_train_tensor = torch.tensor(x_train,  dtype=torch.float32)[:, None, :, :]\n",
        "y_train_tensor = torch.tensor(np.array([labels[y] for y in y_train]),  dtype=torch.long)\n",
        "\n",
        "train_dataset = CustomTensorDataset(tensors=(x_train_tensor, y_train_tensor), transform=trans)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Validation set\n",
        "x_valid_tensor = torch.tensor(x_valid,  dtype=torch.float32)[:, None, :, :]\n",
        "y_valid_tensor = torch.tensor(np.array([labels[y] for y in y_valid]),  dtype=torch.long)\n",
        "\n",
        "valid_dataset = CustomTensorDataset(tensors=(x_valid_tensor, y_valid_tensor), transform=trans)\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is8H3rG-UwQn"
      },
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,  pin_memory = True, batch_size=64, shuffle=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz957W5DNo4z"
      },
      "source": [
        "####Hyperparameter list\n",
        " \n",
        "If new variable added but them in the config file\n",
        "\n",
        "Supported models: AlexNet\n",
        "\n",
        "```\n",
        "\n",
        "hyperparam = {\n",
        "    \"training_data\": train_data,\n",
        "    \"valid_data\": valid_data,\n",
        "\n",
        "    \"model\":\"AlexNet\",\n",
        "    \"learning_rate\":1e-2,\n",
        "    \"batch_size\":64,\n",
        "    \"epochs\": 5,\n",
        "\n",
        "    \"k_folds\" :5,\n",
        "    \"description\":\"Setting up momemtum to 0.95\"\n",
        "    }\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQG92e_caWVh"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALBccyqVKAYd"
      },
      "source": [
        "#This code is an adaptation from Weigth&BiaisesFramework we established a pipeline to track our data : https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb#scrollTo=bZiTlrNkRKzm&uniqifier=1\n",
        "def model_pipeline(hyperparameters,training_data,valid_data,PATH):\n",
        "    \"\"\"\"\n",
        "    \"Hyperparameters need to be a dictionary with all the necessary information\" Wandb librairie will load the data\"\n",
        "    \"into their server.\"\n",
        "    \"\"\"\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(entity=\"xabjuwplb\",project=\"CNN\", config=hyperparameters):\n",
        "\n",
        "        config = wandb.config\n",
        "        train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=config.batch_size, shuffle=True)\n",
        "        valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "        # make the model, data, and optimization problem\n",
        "        model, criterion, optimizer = make(config)\n",
        "        model.to(device)\n",
        "        criterion.to(device)\n",
        "        # and use them to train the model\n",
        "        train(model, train_dataloader,valid_dataloader, criterion, optimizer, config)\n",
        "\n",
        "        # and test its final performance\n",
        "        test(model, valid_dataloader,train_dataloader)\n",
        "        torch.save(model.state_dict(), PATH)\n",
        "\n",
        "    return model\n",
        "\n",
        "def make(config):\n",
        "\n",
        "    # Make the model\n",
        "    model = initialize_model(config)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(), lr=config.learning_rate)    \n",
        "    return model, criterion, optimizer\n",
        "\n",
        "def make_loaders(config):\n",
        "\n",
        "    return train_dataloader, valid_dataloader\n",
        "\n",
        "\n",
        "def initialize_model(config):\n",
        "    #When implmenting add it to the map\n",
        "    if config.model == \"AlexNet\":\n",
        "        return AlexNet()\n",
        "    elif config.model == \"AlexNetv2\":\n",
        "        return AlexNetv2()\n",
        "\n",
        "def train(model, train_loader,valid_loader, criterion, optimizer, config):\n",
        "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "    # Run training and track with wandb\n",
        "    total_batches = len(train_loader) * config.epochs\n",
        "    example_ct = 0  # number of examples seen\n",
        "    batch_ct = 0\n",
        "    loss = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        for _, (image,label) in enumerate(train_loader):\n",
        "            loss = train_batch(image, label, model, optimizer, criterion)\n",
        "            valid_loss = valid_batch(valid_loader,model,criterion)\n",
        "            example_ct +=  len(image)\n",
        "            batch_ct += 1\n",
        "\n",
        "            # Report metrics every 25th batch\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss,valid_loss, example_ct, epoch)\n",
        "        test(model,valid_loader,train_loader)\n",
        "    \n",
        "\n",
        "#\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    \n",
        "    # Forward pass ➡\n",
        "\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass ⬅\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "def valid_batch(valid_data_load,model,criterion):\n",
        "\n",
        "    valid_loss = 0\n",
        "    valid_image,valid_label = next(iter(valid_data_load))\n",
        "\n",
        "    valid_images, valid_labels = valid_image.to(device), valid_label.to(device)\n",
        "    # Forward pass ➡\n",
        "    outputs = model(valid_images)\n",
        "\n",
        "    valid_loss += criterion(outputs, valid_labels)\n",
        "\n",
        "    return valid_loss\n",
        "\n",
        "\n",
        "\n",
        "def test(model, test_loader,train_loader):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        true_y = np.array([])\n",
        "        predicted_y = np.array([])\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            # predicted_y = np.append(predicted_y,predicted.cpu().detach().tolist())\n",
        "            # true_y = np.append(true_y,labels.cpu().detach().tolist())\n",
        "\n",
        "        # f1_score_micro = f1_score(true_y,predicted_y,average=\"micro\")\n",
        "        print(f\"Accuracy of the model on the {total} \" +\n",
        "              f\"test images: {100 * correct / total}%\")\n",
        "\n",
        "        \n",
        "        wandb.log({\"test_accuracy\": correct / total})\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        true_y =np.array([])\n",
        "        predicted_y = np.array([])\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            # true_y= np.append(true_y,labels.cpu().detach().tolist())\n",
        "            # predicted_y=np.append(predicted_y,predicted.cpu().detach().tolist())    \n",
        "\n",
        "        print(f\"Accuracy of the model on the {total} \" +\n",
        "              f\"training images: {100 * correct / total}%\")\n",
        "        wandb.log({\"training_accuracy\": correct / total})\n",
        "        # f1_score_micro = f1_score(true_y,predicted_y,average=\"micro\")\n",
        "        # print(f\"F1 micro score {total} \" +\n",
        "        #       f\"training images: {f1_score_micro}\")        \n",
        "        # wandb.log({\"training_accuracy\": correct / total})\n"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGhTokAs0r3k"
      },
      "source": [
        "def train_log(loss,valid_loss, example_ct, epoch):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"training_loss\": loss,\"valid_loss\":valid_loss}, step=example_ct)\n",
        "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
        "    print(f\"Validation loss after \" + str(example_ct).zfill(5) + f\" examples: {valid_loss:.3f}\")\n",
        "    "
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weutOzrH7eLH"
      },
      "source": [
        "def save_model(epoch,model,optimizer,loss,path):\n",
        "\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"loss\": loss\n",
        "    }, path)\n",
        "###\n",
        "#\n",
        "def load_model(checkpoints_path):\n",
        "    checkpoints = torch.load(PATH)\n",
        "    with wandb.init(entity=\"xabjuwplb\",project=\"CNN\", config=checkpoints):\n",
        "        config = wandb.config \n",
        "        #Initialize\n",
        "        model = initialize_model(config.model)\n",
        "        optimizer = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "        optimizer = optimizer.load_state_dict(config['optimizer_state_dict'])\n",
        "        checkpoint = torch.load(PATH)\n",
        "        model.load_state_dict(config['model_state_dict'])\n",
        "        optimizer.load_state_dict(config['optimizer_state_dict'])\n",
        "        epoch = config['epoch']\n",
        "        loss = config['loss']\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pix4N3fsWKch"
      },
      "source": [
        "config = {\n",
        "\n",
        "    \"model\":\"AlexNetv2\",\n",
        "    \"learning_rate\":1e-2,\n",
        "    \"batch_size\":64,\n",
        "    \"epochs\": 10,\n",
        "\n",
        "    }\n"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "143VH24uDpjn"
      },
      "source": [
        "### Running the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5ecdebda2b9e4633bb4d367ab36d62b5",
            "744330ae08ab4ac887c3b79efd339a53",
            "1fdf06341a93493fb354bb21c4b16d6d",
            "95f08d39b4c44ab6be2d20482c4600f1",
            "2abeac70f21b4daeba99ea5362f40dea",
            "79d18efcd4f84255b2f89f2127e95240",
            "b46bbc12d81c469e967ea0f9656adad1",
            "b1de955c583841779fdd926a7f4dceeb",
            "3025589946844141a684db2189626c8d",
            "f8eee69ad35742a488571b934ee3e063",
            "96a9fd56da3e48af8b0da50c1a99c8a8"
          ]
        },
        "id": "jMrh0Y2MTY2-",
        "outputId": "9f30d885-84e1-4d7f-bc73-37c3e7ec8dee"
      },
      "source": [
        "# Build, train and analyze the model with the pipeline\n",
        "model = model_pipeline(config,train_dataset,valid_dataset,\"model.pth\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/xabjuwplb/CNN/runs/2793vxv4\" target=\"_blank\">silver-snow-56</a></strong> to <a href=\"https://wandb.ai/xabjuwplb/CNN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AlexNetv2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ecdebda2b9e4633bb4d367ab36d62b5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after 01536 examples: 2.335\n",
            "Validation loss after 01536 examples: 2.247\n",
            "Loss after 03136 examples: 2.388\n",
            "Validation loss after 03136 examples: 2.322\n",
            "Loss after 04736 examples: 2.395\n",
            "Validation loss after 04736 examples: 2.327\n",
            "Loss after 06336 examples: 2.324\n",
            "Validation loss after 06336 examples: 2.350\n",
            "Loss after 07936 examples: 2.264\n",
            "Validation loss after 07936 examples: 2.252\n",
            "Loss after 09536 examples: 2.361\n",
            "Validation loss after 09536 examples: 2.317\n",
            "Loss after 11136 examples: 2.302\n",
            "Validation loss after 11136 examples: 2.251\n",
            "Loss after 12736 examples: 2.335\n",
            "Validation loss after 12736 examples: 2.262\n",
            "Loss after 14336 examples: 2.337\n",
            "Validation loss after 14336 examples: 2.264\n",
            "Loss after 15936 examples: 2.175\n",
            "Validation loss after 15936 examples: 2.314\n",
            "Loss after 17536 examples: 2.129\n",
            "Validation loss after 17536 examples: 2.388\n",
            "Loss after 19136 examples: 2.294\n",
            "Validation loss after 19136 examples: 2.249\n",
            "Loss after 20736 examples: 2.149\n",
            "Validation loss after 20736 examples: 2.001\n",
            "Loss after 22336 examples: 2.131\n",
            "Validation loss after 22336 examples: 2.163\n",
            "Loss after 23936 examples: 2.343\n",
            "Validation loss after 23936 examples: 2.230\n",
            "Loss after 25536 examples: 2.261\n",
            "Validation loss after 25536 examples: 2.262\n",
            "Loss after 27136 examples: 2.260\n",
            "Validation loss after 27136 examples: 2.162\n",
            "Loss after 28736 examples: 2.255\n",
            "Validation loss after 28736 examples: 2.307\n",
            "Loss after 30336 examples: 2.150\n",
            "Validation loss after 30336 examples: 2.109\n",
            "Loss after 31936 examples: 2.575\n",
            "Validation loss after 31936 examples: 2.451\n",
            "Loss after 33536 examples: 2.133\n",
            "Validation loss after 33536 examples: 2.083\n",
            "Loss after 35136 examples: 2.075\n",
            "Validation loss after 35136 examples: 2.101\n",
            "Loss after 36736 examples: 1.945\n",
            "Validation loss after 36736 examples: 2.134\n",
            "Loss after 38336 examples: 2.167\n",
            "Validation loss after 38336 examples: 2.194\n",
            "Loss after 39936 examples: 2.040\n",
            "Validation loss after 39936 examples: 1.943\n",
            "Loss after 41536 examples: 2.017\n",
            "Validation loss after 41536 examples: 1.941\n",
            "Loss after 43136 examples: 2.009\n",
            "Validation loss after 43136 examples: 2.321\n",
            "Loss after 44736 examples: 2.036\n",
            "Validation loss after 44736 examples: 2.029\n",
            "Loss after 46336 examples: 1.880\n",
            "Validation loss after 46336 examples: 1.955\n",
            "Loss after 47936 examples: 2.161\n",
            "Validation loss after 47936 examples: 2.139\n",
            "Loss after 49536 examples: 1.838\n",
            "Validation loss after 49536 examples: 2.084\n",
            "Accuracy of the model on the 1886 test images: 29.003181336161187%\n",
            "Accuracy of the model on the 50000 training images: 29.826%\n",
            "Loss after 51088 examples: 2.042\n",
            "Validation loss after 51088 examples: 1.910\n",
            "Loss after 52688 examples: 2.040\n",
            "Validation loss after 52688 examples: 1.949\n",
            "Loss after 54288 examples: 1.971\n",
            "Validation loss after 54288 examples: 1.852\n",
            "Loss after 55888 examples: 1.929\n",
            "Validation loss after 55888 examples: 1.767\n",
            "Loss after 57488 examples: 1.622\n",
            "Validation loss after 57488 examples: 1.709\n",
            "Loss after 59088 examples: 2.218\n",
            "Validation loss after 59088 examples: 1.819\n",
            "Loss after 60688 examples: 1.999\n",
            "Validation loss after 60688 examples: 1.855\n",
            "Loss after 62288 examples: 2.070\n",
            "Validation loss after 62288 examples: 2.322\n",
            "Loss after 63888 examples: 1.901\n",
            "Validation loss after 63888 examples: 2.103\n",
            "Loss after 65488 examples: 1.759\n",
            "Validation loss after 65488 examples: 2.149\n",
            "Loss after 67088 examples: 1.851\n",
            "Validation loss after 67088 examples: 1.656\n",
            "Loss after 68688 examples: 1.884\n",
            "Validation loss after 68688 examples: 1.715\n",
            "Loss after 70288 examples: 1.813\n",
            "Validation loss after 70288 examples: 1.815\n",
            "Loss after 71888 examples: 1.805\n",
            "Validation loss after 71888 examples: 1.743\n",
            "Loss after 73488 examples: 1.884\n",
            "Validation loss after 73488 examples: 1.779\n",
            "Loss after 75088 examples: 1.720\n",
            "Validation loss after 75088 examples: 1.680\n",
            "Loss after 76688 examples: 1.811\n",
            "Validation loss after 76688 examples: 1.880\n",
            "Loss after 78288 examples: 1.704\n",
            "Validation loss after 78288 examples: 1.830\n",
            "Loss after 79888 examples: 1.966\n",
            "Validation loss after 79888 examples: 1.829\n",
            "Loss after 81488 examples: 1.524\n",
            "Validation loss after 81488 examples: 2.027\n",
            "Loss after 83088 examples: 1.593\n",
            "Validation loss after 83088 examples: 1.701\n",
            "Loss after 84688 examples: 1.855\n",
            "Validation loss after 84688 examples: 1.774\n",
            "Loss after 86288 examples: 1.692\n",
            "Validation loss after 86288 examples: 1.950\n",
            "Loss after 87888 examples: 1.833\n",
            "Validation loss after 87888 examples: 1.912\n",
            "Loss after 89488 examples: 1.805\n",
            "Validation loss after 89488 examples: 1.597\n",
            "Loss after 91088 examples: 1.681\n",
            "Validation loss after 91088 examples: 1.551\n",
            "Loss after 92688 examples: 1.564\n",
            "Validation loss after 92688 examples: 1.869\n",
            "Loss after 94288 examples: 1.704\n",
            "Validation loss after 94288 examples: 1.551\n",
            "Loss after 95888 examples: 1.656\n",
            "Validation loss after 95888 examples: 1.651\n",
            "Loss after 97488 examples: 1.750\n",
            "Validation loss after 97488 examples: 1.783\n",
            "Loss after 99088 examples: 1.960\n",
            "Validation loss after 99088 examples: 1.877\n",
            "Accuracy of the model on the 1886 test images: 37.69883351007423%\n",
            "Accuracy of the model on the 50000 training images: 35.786%\n",
            "Loss after 100640 examples: 1.603\n",
            "Validation loss after 100640 examples: 1.404\n",
            "Loss after 102240 examples: 1.861\n",
            "Validation loss after 102240 examples: 1.803\n",
            "Loss after 103840 examples: 1.715\n",
            "Validation loss after 103840 examples: 1.493\n",
            "Loss after 105440 examples: 1.549\n",
            "Validation loss after 105440 examples: 1.914\n",
            "Loss after 107040 examples: 1.761\n",
            "Validation loss after 107040 examples: 1.953\n",
            "Loss after 108640 examples: 1.615\n",
            "Validation loss after 108640 examples: 1.634\n",
            "Loss after 110240 examples: 1.951\n",
            "Validation loss after 110240 examples: 1.587\n",
            "Loss after 111840 examples: 1.761\n",
            "Validation loss after 111840 examples: 1.798\n",
            "Loss after 113440 examples: 1.512\n",
            "Validation loss after 113440 examples: 1.582\n",
            "Loss after 115040 examples: 1.740\n",
            "Validation loss after 115040 examples: 1.631\n",
            "Loss after 116640 examples: 1.815\n",
            "Validation loss after 116640 examples: 1.808\n",
            "Loss after 118240 examples: 1.654\n",
            "Validation loss after 118240 examples: 1.945\n",
            "Loss after 119840 examples: 1.664\n",
            "Validation loss after 119840 examples: 1.875\n",
            "Loss after 121440 examples: 1.629\n",
            "Validation loss after 121440 examples: 1.684\n",
            "Loss after 123040 examples: 1.543\n",
            "Validation loss after 123040 examples: 1.361\n",
            "Loss after 124640 examples: 1.471\n",
            "Validation loss after 124640 examples: 1.467\n",
            "Loss after 126240 examples: 1.502\n",
            "Validation loss after 126240 examples: 1.380\n",
            "Loss after 127840 examples: 1.504\n",
            "Validation loss after 127840 examples: 1.530\n",
            "Loss after 129440 examples: 1.510\n",
            "Validation loss after 129440 examples: 1.609\n",
            "Loss after 131040 examples: 1.427\n",
            "Validation loss after 131040 examples: 1.354\n",
            "Loss after 132640 examples: 1.993\n",
            "Validation loss after 132640 examples: 1.379\n",
            "Loss after 134240 examples: 1.630\n",
            "Validation loss after 134240 examples: 1.401\n",
            "Loss after 135840 examples: 1.353\n",
            "Validation loss after 135840 examples: 1.626\n",
            "Loss after 137440 examples: 1.527\n",
            "Validation loss after 137440 examples: 1.452\n",
            "Loss after 139040 examples: 1.832\n",
            "Validation loss after 139040 examples: 1.572\n",
            "Loss after 140640 examples: 1.549\n",
            "Validation loss after 140640 examples: 1.670\n",
            "Loss after 142240 examples: 1.678\n",
            "Validation loss after 142240 examples: 1.417\n",
            "Loss after 143840 examples: 1.255\n",
            "Validation loss after 143840 examples: 1.356\n",
            "Loss after 145440 examples: 1.515\n",
            "Validation loss after 145440 examples: 1.618\n",
            "Loss after 147040 examples: 1.417\n",
            "Validation loss after 147040 examples: 1.506\n",
            "Loss after 148640 examples: 1.383\n",
            "Validation loss after 148640 examples: 1.050\n",
            "Accuracy of the model on the 1886 test images: 38.17603393425239%\n",
            "Accuracy of the model on the 50000 training images: 38.306%\n",
            "Loss after 150192 examples: 1.403\n",
            "Validation loss after 150192 examples: 1.202\n",
            "Loss after 151792 examples: 1.526\n",
            "Validation loss after 151792 examples: 1.563\n",
            "Loss after 153392 examples: 1.363\n",
            "Validation loss after 153392 examples: 1.619\n",
            "Loss after 154992 examples: 1.507\n",
            "Validation loss after 154992 examples: 1.596\n",
            "Loss after 156592 examples: 1.584\n",
            "Validation loss after 156592 examples: 1.078\n",
            "Loss after 158192 examples: 1.195\n",
            "Validation loss after 158192 examples: 1.669\n",
            "Loss after 159792 examples: 1.435\n",
            "Validation loss after 159792 examples: 1.371\n",
            "Loss after 161392 examples: 1.330\n",
            "Validation loss after 161392 examples: 1.770\n",
            "Loss after 162992 examples: 1.192\n",
            "Validation loss after 162992 examples: 1.268\n",
            "Loss after 164592 examples: 1.512\n",
            "Validation loss after 164592 examples: 1.452\n",
            "Loss after 166192 examples: 1.255\n",
            "Validation loss after 166192 examples: 1.414\n",
            "Loss after 167792 examples: 1.603\n",
            "Validation loss after 167792 examples: 1.305\n",
            "Loss after 169392 examples: 1.461\n",
            "Validation loss after 169392 examples: 1.622\n",
            "Loss after 170992 examples: 1.625\n",
            "Validation loss after 170992 examples: 1.441\n",
            "Loss after 172592 examples: 1.441\n",
            "Validation loss after 172592 examples: 1.225\n",
            "Loss after 174192 examples: 1.562\n",
            "Validation loss after 174192 examples: 1.319\n",
            "Loss after 175792 examples: 1.413\n",
            "Validation loss after 175792 examples: 1.436\n",
            "Loss after 177392 examples: 1.299\n",
            "Validation loss after 177392 examples: 1.310\n",
            "Loss after 178992 examples: 1.207\n",
            "Validation loss after 178992 examples: 1.326\n",
            "Loss after 180592 examples: 1.584\n",
            "Validation loss after 180592 examples: 1.684\n",
            "Loss after 182192 examples: 1.268\n",
            "Validation loss after 182192 examples: 1.274\n",
            "Loss after 183792 examples: 1.425\n",
            "Validation loss after 183792 examples: 1.233\n",
            "Loss after 185392 examples: 1.396\n",
            "Validation loss after 185392 examples: 1.433\n",
            "Loss after 186992 examples: 1.346\n",
            "Validation loss after 186992 examples: 1.384\n",
            "Loss after 188592 examples: 1.576\n",
            "Validation loss after 188592 examples: 1.506\n",
            "Loss after 190192 examples: 1.488\n",
            "Validation loss after 190192 examples: 1.231\n",
            "Loss after 191792 examples: 1.370\n",
            "Validation loss after 191792 examples: 1.606\n",
            "Loss after 193392 examples: 1.540\n",
            "Validation loss after 193392 examples: 1.410\n",
            "Loss after 194992 examples: 1.758\n",
            "Validation loss after 194992 examples: 1.474\n",
            "Loss after 196592 examples: 1.340\n",
            "Validation loss after 196592 examples: 1.476\n",
            "Loss after 198192 examples: 1.254\n",
            "Validation loss after 198192 examples: 1.206\n",
            "Loss after 199792 examples: 1.511\n",
            "Validation loss after 199792 examples: 1.635\n",
            "Accuracy of the model on the 1886 test images: 37.539766702014845%\n",
            "Accuracy of the model on the 50000 training images: 38.126%\n",
            "Loss after 201344 examples: 1.350\n",
            "Validation loss after 201344 examples: 1.371\n",
            "Loss after 202944 examples: 1.759\n",
            "Validation loss after 202944 examples: 1.382\n",
            "Loss after 204544 examples: 1.273\n",
            "Validation loss after 204544 examples: 1.183\n",
            "Loss after 206144 examples: 1.094\n",
            "Validation loss after 206144 examples: 1.023\n",
            "Loss after 207744 examples: 1.431\n",
            "Validation loss after 207744 examples: 1.419\n",
            "Loss after 209344 examples: 1.172\n",
            "Validation loss after 209344 examples: 1.347\n",
            "Loss after 210944 examples: 1.425\n",
            "Validation loss after 210944 examples: 1.836\n",
            "Loss after 212544 examples: 1.453\n",
            "Validation loss after 212544 examples: 1.643\n",
            "Loss after 214144 examples: 1.232\n",
            "Validation loss after 214144 examples: 1.667\n",
            "Loss after 215744 examples: 1.569\n",
            "Validation loss after 215744 examples: 1.228\n",
            "Loss after 217344 examples: 1.195\n",
            "Validation loss after 217344 examples: 1.233\n",
            "Loss after 218944 examples: 0.986\n",
            "Validation loss after 218944 examples: 1.246\n",
            "Loss after 220544 examples: 1.160\n",
            "Validation loss after 220544 examples: 1.206\n",
            "Loss after 222144 examples: 1.198\n",
            "Validation loss after 222144 examples: 1.623\n",
            "Loss after 223744 examples: 1.263\n",
            "Validation loss after 223744 examples: 1.430\n",
            "Loss after 225344 examples: 1.261\n",
            "Validation loss after 225344 examples: 1.173\n",
            "Loss after 226944 examples: 2.291\n",
            "Validation loss after 226944 examples: 2.164\n",
            "Loss after 228544 examples: 1.285\n",
            "Validation loss after 228544 examples: 1.388\n",
            "Loss after 230144 examples: 1.427\n",
            "Validation loss after 230144 examples: 1.454\n",
            "Loss after 231744 examples: 1.384\n",
            "Validation loss after 231744 examples: 1.509\n",
            "Loss after 233344 examples: 0.914\n",
            "Validation loss after 233344 examples: 1.249\n",
            "Loss after 234944 examples: 1.023\n",
            "Validation loss after 234944 examples: 1.292\n",
            "Loss after 236544 examples: 1.250\n",
            "Validation loss after 236544 examples: 1.271\n",
            "Loss after 238144 examples: 1.343\n",
            "Validation loss after 238144 examples: 1.029\n",
            "Loss after 239744 examples: 1.331\n",
            "Validation loss after 239744 examples: 1.221\n",
            "Loss after 241344 examples: 1.477\n",
            "Validation loss after 241344 examples: 1.262\n",
            "Loss after 242944 examples: 1.294\n",
            "Validation loss after 242944 examples: 1.028\n",
            "Loss after 244544 examples: 1.234\n",
            "Validation loss after 244544 examples: 1.352\n",
            "Loss after 246144 examples: 1.125\n",
            "Validation loss after 246144 examples: 1.019\n",
            "Loss after 247744 examples: 1.057\n",
            "Validation loss after 247744 examples: 1.504\n",
            "Loss after 249344 examples: 1.271\n",
            "Validation loss after 249344 examples: 1.291\n",
            "Accuracy of the model on the 1886 test images: 39.44856839872747%\n",
            "Accuracy of the model on the 50000 training images: 39.98%\n",
            "Loss after 250896 examples: 1.237\n",
            "Validation loss after 250896 examples: 1.681\n",
            "Loss after 252496 examples: 1.230\n",
            "Validation loss after 252496 examples: 1.115\n",
            "Loss after 254096 examples: 1.279\n",
            "Validation loss after 254096 examples: 1.019\n",
            "Loss after 255696 examples: 1.412\n",
            "Validation loss after 255696 examples: 1.281\n",
            "Loss after 257296 examples: 1.338\n",
            "Validation loss after 257296 examples: 2.032\n",
            "Loss after 258896 examples: 1.015\n",
            "Validation loss after 258896 examples: 1.265\n",
            "Loss after 260496 examples: 0.835\n",
            "Validation loss after 260496 examples: 1.173\n",
            "Loss after 262096 examples: 0.904\n",
            "Validation loss after 262096 examples: 1.359\n",
            "Loss after 263696 examples: 1.004\n",
            "Validation loss after 263696 examples: 1.179\n",
            "Loss after 265296 examples: 1.141\n",
            "Validation loss after 265296 examples: 1.239\n",
            "Loss after 266896 examples: 0.927\n",
            "Validation loss after 266896 examples: 1.088\n",
            "Loss after 268496 examples: 1.322\n",
            "Validation loss after 268496 examples: 1.076\n",
            "Loss after 270096 examples: 0.952\n",
            "Validation loss after 270096 examples: 1.623\n",
            "Loss after 271696 examples: 0.973\n",
            "Validation loss after 271696 examples: 1.297\n",
            "Loss after 273296 examples: 1.168\n",
            "Validation loss after 273296 examples: 1.450\n",
            "Loss after 274896 examples: 1.280\n",
            "Validation loss after 274896 examples: 1.452\n",
            "Loss after 276496 examples: 0.960\n",
            "Validation loss after 276496 examples: 1.564\n",
            "Loss after 278096 examples: 1.182\n",
            "Validation loss after 278096 examples: 1.526\n",
            "Loss after 279696 examples: 1.015\n",
            "Validation loss after 279696 examples: 1.409\n",
            "Loss after 281296 examples: 1.022\n",
            "Validation loss after 281296 examples: 1.218\n",
            "Loss after 282896 examples: 1.059\n",
            "Validation loss after 282896 examples: 1.306\n",
            "Loss after 284496 examples: 1.379\n",
            "Validation loss after 284496 examples: 1.591\n",
            "Loss after 286096 examples: 0.875\n",
            "Validation loss after 286096 examples: 0.942\n",
            "Loss after 287696 examples: 1.249\n",
            "Validation loss after 287696 examples: 1.160\n",
            "Loss after 289296 examples: 0.834\n",
            "Validation loss after 289296 examples: 0.845\n",
            "Loss after 290896 examples: 1.072\n",
            "Validation loss after 290896 examples: 1.031\n",
            "Loss after 292496 examples: 0.911\n",
            "Validation loss after 292496 examples: 1.089\n",
            "Loss after 294096 examples: 0.895\n",
            "Validation loss after 294096 examples: 1.239\n",
            "Loss after 295696 examples: 0.774\n",
            "Validation loss after 295696 examples: 0.894\n",
            "Loss after 297296 examples: 1.409\n",
            "Validation loss after 297296 examples: 1.564\n",
            "Loss after 298896 examples: 1.052\n",
            "Validation loss after 298896 examples: 1.273\n",
            "Accuracy of the model on the 1886 test images: 47.667020148462356%\n",
            "Accuracy of the model on the 50000 training images: 55.75%\n",
            "Loss after 300448 examples: 0.927\n",
            "Validation loss after 300448 examples: 0.832\n",
            "Loss after 302048 examples: 0.848\n",
            "Validation loss after 302048 examples: 1.442\n",
            "Loss after 303648 examples: 0.695\n",
            "Validation loss after 303648 examples: 1.704\n",
            "Loss after 305248 examples: 1.012\n",
            "Validation loss after 305248 examples: 1.304\n",
            "Loss after 306848 examples: 1.074\n",
            "Validation loss after 306848 examples: 1.335\n",
            "Loss after 308448 examples: 0.798\n",
            "Validation loss after 308448 examples: 1.448\n",
            "Loss after 310048 examples: 0.972\n",
            "Validation loss after 310048 examples: 1.099\n",
            "Loss after 311648 examples: 0.953\n",
            "Validation loss after 311648 examples: 2.101\n",
            "Loss after 313248 examples: 0.987\n",
            "Validation loss after 313248 examples: 1.504\n",
            "Loss after 314848 examples: 1.003\n",
            "Validation loss after 314848 examples: 1.380\n",
            "Loss after 316448 examples: 0.653\n",
            "Validation loss after 316448 examples: 1.318\n",
            "Loss after 318048 examples: 0.600\n",
            "Validation loss after 318048 examples: 1.177\n",
            "Loss after 319648 examples: 0.579\n",
            "Validation loss after 319648 examples: 1.559\n",
            "Loss after 321248 examples: 0.637\n",
            "Validation loss after 321248 examples: 1.175\n",
            "Loss after 322848 examples: 1.005\n",
            "Validation loss after 322848 examples: 1.467\n",
            "Loss after 324448 examples: 1.307\n",
            "Validation loss after 324448 examples: 1.321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR2h9sO10r3Z"
      },
      "source": [
        "def reset_weights(m):\n",
        "  '''\n",
        "    Try resetting model weights to avoid\n",
        "    weight leakage.\n",
        "  '''\n",
        "  for layer in m.children():\n",
        "   if hasattr(layer, 'reset_parameters'):\n",
        "    print(f'Reset trainable parameters of layer = {layer}')\n",
        "    layer.reset_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pvpRIX5dcm7"
      },
      "source": [
        "\"\"\"\n",
        "\" config : Map with the hyperparameter\n",
        "\" model : model pass in parameter:\n",
        "\" training_dataset: traning_dataset that will be split in cross_validatin\n",
        "\" test_set : untouched dataset\n",
        "\"\n",
        "\"\"\"\n",
        "\n",
        "def train_model(configs,model,test_set):\n",
        "\n",
        "    #Hyper_parameter settings\n",
        "    kfold = KFold(n_splits=config[\"hyperparameter\"][\"k_folds\"], shuffle=True)\n",
        "    batch_size = config[\"hyperparameter\"][\"batch_size\"]\n",
        "    num_epochs = config[\"hyperparameter\"][\"num_epochs\"]\n",
        "    pooling_filter_size = config[\"hyperparameter\"][\"pooling_filter_size\"]\n",
        "    pooling_stride = config[\"hyperparameter\"][\"pooling_stride\"]\n",
        "    optimizer = config[\"hyperparameter\"][\"optimizer\"]\n",
        "    \n",
        "    plot_map ={\n",
        "        \"training\": [],\n",
        "        \"validation\": [],\n",
        "        \"test\": []\n",
        "    }\n",
        "\n",
        "    #Change the model here for the moment\n",
        "    network = AlexNet(pooling_filter_size,pooling_stride).to(device)\n",
        "\n",
        "    validation_data,validation_label = test_set \n",
        "    #Validation set\n",
        "    inputs = inputs.to(device).float()\n",
        "    labels = labels.to(device).long()\n",
        "\n",
        "    #Label\n",
        "    for fold, (train_ids, test_ids) in enumerate(kfold.split(training_dataset)):\n",
        "        training_loss_iteration = []\n",
        "        torch.manual_seed(np.random.rand()*100)\n",
        "        # Print\n",
        "        print(f'FOLD {fold}')\n",
        "        print('--------------------------------')\n",
        "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "        \n",
        "        \n",
        "        # Define data loaders for training and testing data in this fold\n",
        "        trainloader = torch.utils.data.DataLoader(\n",
        "                        train_dataset, \n",
        "                        batch_size=batch_size, sampler=train_subsampler)\n",
        "        testloader = torch.utils.data.DataLoader(\n",
        "                        train_dataset,\n",
        "                        batch_size=batch_size, sampler=test_subsampler)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        network = AlexNet().to(device)\n",
        "        network.apply(reset_weights)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.2)\n",
        "\n",
        "        \n",
        "        for epoch in range(num_epochs):\n",
        "            # Print epoch\n",
        "            print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "            # Set current loss value\n",
        "            current_loss = 0.0\n",
        "            for i, data in enumerate(trainloader):\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(device).float()\n",
        "                labels = labels.to(device).long()\n",
        "                optimizer.zero_grad()\n",
        "                outputs = network(inputs)  # forward pass \n",
        "                outputs = outputs.float()\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                loss_array.append(float(loss))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                current_loss += loss.item()\n",
        "\n",
        "                if i % printfreq == printfreq - 1 :\n",
        "                    print('Training loss after mini-batch %5d: %.3f' % (i + 1, current_loss / printfreq))\n",
        "                    current_loss = 0.0\n",
        "                training_loss_iteration.append(loss.item())\n",
        "        \n",
        "        print('Training process has finished. Saving trained model.')\n",
        "\n",
        "        # Print about testing\n",
        "        print('Starting testing')\n",
        "\n",
        "        # Saving the model\n",
        "        save_path = f'./model-fold-{fold}.pth'\n",
        "        torch.save(network.state_dict(), save_path)\n",
        "\n",
        "    plot_map[\"training_set\"].append(training_loss_iteration)\n",
        "    plot_map[\"test_set\"].append(test_loss_iteration)\n",
        "    plot_map[\"validation\"].append()\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pmUkZs94Anj"
      },
      "source": [
        "batch_size/len(train_subsampler)*len(test_subsampler)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8iEHIwV0r3l"
      },
      "source": [
        "batch_size = 64\n",
        "num_epochs = 15\n",
        "\n",
        "#pooling\n",
        "pooling_filter_size = 2\n",
        "pooling_stride = 2\n",
        "\n",
        "#k-fold-validation\n",
        "k_folds = 5\n",
        "\n",
        "# For fold results\n",
        "results = {}\n",
        "\n",
        "# Set fixed random number seed\n",
        "\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "running_loss = 0 \n",
        "printfreq = 25\n",
        "\n",
        "network = AlexNet(pooling_filter_size,pooling_stride).to(device)\n",
        "network.apply(reset_weights)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n",
        "    torch.manual_seed(np.random.rand()*100)\n",
        "    # Print\n",
        "    print(f'FOLD {fold}')\n",
        "    print('--------------------------------')\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "    \n",
        "    # Define data loaders for training and testing data in this fold\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "                      train_dataset, \n",
        "                      batch_size=batch_size, sampler=train_subsampler)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "                      train_dataset,\n",
        "                      batch_size=round(batch_size/len(train_subsampler)*len(test_subsampler)), sampler=test_subsampler)\n",
        "    assert len(testloader) == len(trainloader)\n",
        "    network = AlexNet(pooling_filter_size,pooling_stride).to(device)\n",
        "    network.apply(reset_weights)\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion_test = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)\n",
        "    \n",
        "    epoch_loss = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Print epoch\n",
        "        print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "        # Set current loss value\n",
        "        current_loss = 0.0\n",
        "        current_loss_test = 0.0\n",
        "        loss_array = []\n",
        "        for i, (data_train,data_test) in enumerate(zip(trainloader,testloader)):\n",
        "            inputs, labels = data_train\n",
        "            inputs = inputs.to(device).float()\n",
        "            labels = labels.to(device).long()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = network(inputs)  # forward pass \n",
        "            outputs = outputs.float()\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss_array.append(float(loss))\n",
        "\n",
        "            inputs_test, labels_test = data_test\n",
        "            inputs_test = inputs_test.to(device).float()\n",
        "            labels_test = labels_test.to(device).long()\n",
        "            optimizer.zero_grad()\n",
        "            outputs_test = network(inputs_test)\n",
        "            outputs_test = outputs_test.float()\n",
        "            loss_test = criterion(outputs_test, labels_test)\n",
        "            loss_array.append(float(loss_test))\n",
        "            \n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            current_loss += loss.item()\n",
        "            current_loss_test +=loss_test.item()\n",
        "            if i % printfreq == printfreq - 1 :\n",
        "                print('Avg loss after mini-batch %5d | Training: : %.3f Test Loss %.3f' % (i + 1, current_loss / printfreq, current_loss_test / printfreq))\n",
        "                current_loss = 0.0\n",
        "                current_loss_test = 0.0\n",
        "         # Process is complete.\n",
        "    print('Training process has finished. Saving trained model.')\n",
        "\n",
        "    # Print about testing\n",
        "    print('Starting testing')\n",
        "    \n",
        "    # Saving the model\n",
        "    save_path = f'./model-fold-{fold}.pth'\n",
        "    torch.save(network.state_dict(), save_path)\n",
        "    \n",
        "    # Evaluationfor this fold\n",
        "    correct_training, total_training,correct_testfold, correct_testfold,total_testfold = 0, 0,0,0\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Iterate over the test data and generate predictions\n",
        "      for i, data in enumerate(testloader):\n",
        "\n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "        \n",
        "        inputs = inputs.float().to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Generate outputs\n",
        "        outputs = network(inputs)\n",
        "\n",
        "        # Set total and correct\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "      # Print accuracy\n",
        "      print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
        "      print('--------------------------------')\n",
        "      results[fold] = 100.0 * (correct / total)\n",
        "    # Evaluationfor this fold\n",
        "    correct, total = 0, 0\n",
        "\n",
        "\n",
        "# Print fold results\n",
        "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n",
        "print('--------------------------------')\n",
        "sum = 0.0\n",
        "for key, value in results.items():\n",
        "    print(f'Fold {key}: {value} %')\n",
        "    sum += value\n",
        "print(f'Average: {sum/len(results.items())} %')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKzDZEkS9zFT"
      },
      "source": [
        "      # Iterate over the test data and generate predictions\n",
        "      for i, data in enumerate(testloader):\n",
        "\n",
        "        # Get inputs\n",
        "        inputs, targets = data\n",
        "        \n",
        "        inputs = inputs.float().to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Generate outputs\n",
        "        outputs = network(inputs)\n",
        "\n",
        "        # Set total and correct\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "      # Print accuracy\n",
        "      print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
        "      print('--------------------------------')\n",
        "      results[fold] = 100.0 * (correct / total)\n",
        "    # Evaluationfor this fold\n",
        "    correct, total = 0, 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6PdDKD70r3m"
      },
      "source": [
        "network = AlexNet()\n",
        "network.load_state_dict(torch.load('./model-fold-0.pth'))\n",
        "network.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrsm48eV0r3m"
      },
      "source": [
        "x_test_tensor = torch.tensor(x_test, dtype=float)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=float)\n",
        "\n",
        "test_dataset = CustomTensorDataset(tensors=(x_test_tensor, y_test_tensor), transform=trans)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXobeRYI0r3n"
      },
      "source": [
        "numberOfBatches = 0\n",
        "totalAccuracy = 0\n",
        "\n",
        "for i, data in enumerate(test_dataloader):\n",
        "    numberOfBatches += 1\n",
        "    inputs, labels = data\n",
        "    \n",
        "    outputs = network(inputs.float())\n",
        "    \n",
        "    print(outputs.shape)\n",
        "\n",
        "    preds = outputs.argmax(dim=1).cpu()\n",
        "    \n",
        "    print(preds.shape)\n",
        "    print(preds)\n",
        "    print(labels.shape)\n",
        "    print(labels)\n",
        "    print(\"\")\n",
        "\n",
        "    totalAccuracy += accuracy_score(labels.cpu(), preds.cpu())\n",
        "\n",
        "print(totalAccuracy / numberOfBatches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLmDqLcU0r3n"
      },
      "source": [
        "def make_submission(x_test, network):\n",
        "    header = [\"Id\", \"class\"]\n",
        "    id_value = 0\n",
        "    \n",
        "    classifications =[]\n",
        "    for image in tqdm.tqdm(x_test):\n",
        "        image = torch.reshape(image, (1,1,96, 96))\n",
        "        classifications.append([id_value, network(image.float()).argmax(dim=1)[0].item()])\n",
        "        id_value += 1\n",
        "    \n",
        "    y = PATH+\"./submission.csv\"\n",
        "    with open(y, 'w', newline='') as work:\n",
        "        z = csv.writer(work)\n",
        "        z.writerow(header)\n",
        "        z.writerows(classifications)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u05F87lwr-N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXGpxWDv0r3o"
      },
      "source": [
        "x_test_scalar =  np.array(pkl.load(open(PATH+'x_test.pkl', 'rb')))\n",
        "x_test = x_test_scalar\n",
        "x_test = torch.tensor(x_test, dtype=float)[:, None, :, :]\n",
        "\n",
        "network = AlexNet()\n",
        "network.load_state_dict(torch.load('./model-fold-0.pth'))\n",
        "network.eval()\n",
        "\n",
        "make_submission(x_test, network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmiiJmjZvUrA"
      },
      "source": [
        "def make_submission(x_test, network):\n",
        "    header = [\"Id\", \"class\"]\n",
        "    id_value = 0\n",
        "\n",
        "    classifications =[]\n",
        "    for image in tqdm.tqdm(x_test):\n",
        "        image = torch.reshape(image[0], (1,1,96, 96))\n",
        "        classifications.append([id_value, network(image.float()).argmax(dim=1)[0].item()])\n",
        "        id_value += 1\n",
        "\n",
        "    y = PATH+\"./submission_3dec.csv\"\n",
        "    with open(y, 'w', newline='') as work:\n",
        "        z = csv.writer(work)\n",
        "        z.writerow(header)\n",
        "        z.writerows(classifications)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQhJ6tEPvMMV"
      },
      "source": [
        "x_test_scalar =  np.array(pkl.load(open(PATH+'x_test.pkl', 'rb')))\n",
        "x_test = x_test_scalar\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=float)[:, None, :, :].to(device)\n",
        "y_test = torch.tensor(np.array([1 for y in x_test]), dtype=float)\n",
        "\n",
        "trans = transforms.Compose(\n",
        "    [transforms.Normalize((0.5), (0.5))]\n",
        ")\n",
        "test_dataset = CustomTensorDataset(tensors=(x_test, y_test), transform=trans)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1,\n",
        "                                          shuffle=False)\n",
        "\n",
        "network = AlexNet(pooling_filter_size, pooling_stride).to(device)\n",
        "network.load_state_dict(torch.load('./model-fold-0.pth'))\n",
        "network.eval()\n",
        "\n",
        "make_submission(test_dataloader, network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drQmV_PBvTcK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veuQSV0X0r3p"
      },
      "source": [
        "### Model Graveyard\n",
        "# First model implemented:\n",
        "# class Net(nn.Module): \n",
        "#     def __init__(self): \n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(1,3,3)\n",
        "#         # we use the maxpool multiple times, but define it once\n",
        "#         self.pool = nn.MaxPool2d(pooling_filter_size, pooling_stride)\n",
        "#         # in_channels = 6 because self.conv1 output 6 channel\n",
        "#         self.conv2 = nn.Conv2d(3,6,5)\n",
        "#         self.conv3 = nn.Conv2d(6,16,7)\n",
        "#         # 5*5 comes from the dimension of the last convnet layer\n",
        "#         self.fc1 = nn.Linear(16*7*7, 240) \n",
        "#         self.fc2 = nn.Linear(240, 120)\n",
        "#         self.fc3 = nn.Linear(120, 60)\n",
        "#         self.fc4 = nn.Linear(60, 11)\n",
        "        \n",
        "#     def forward(self, x): \n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         x = self.pool(F.relu(self.conv3(x)))\n",
        "#         x = x.view(-1, 16*7*7)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = F.relu(self.fc3(x))\n",
        "#         x = self.fc4(x)  # no activation on final layer \n",
        "#         return x\n",
        "\n",
        "# net = Net().to(device)\n",
        "\n",
        "# class Net(nn.Module): \n",
        "#     def __init__(self): \n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(1, 3, 5)\n",
        "#         # we use the maxpool multiple times, but define it once\n",
        "#         self.pool = nn.MaxPool2d(pooling_filter_size, pooling_stride)\n",
        "#         # in_channels = 6 because self.conv1 output 6 channel\n",
        "#         self.conv2 = nn.Conv2d(3,6,5) \n",
        "#         # 5*5 comes from the dimension of the last convnet layer\n",
        "#         self.fc1 = nn.Linear(6*21*21, 240) \n",
        "#         self.fc2 = nn.Linear(240, 120)\n",
        "#         self.fc3 = nn.Linear(120, 60)\n",
        "#         self.fc4 = nn.Linear(60, 11)\n",
        "        \n",
        "#     def forward(self, x): \n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         x = x.view(-1, 6*21*21)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = F.relu(self.fc3(x))\n",
        "#         x = self.fc4(x)  # no activation on final layer \n",
        "#         return x\n",
        "\n",
        "# net = Net().to(device)\n",
        "\n",
        "# class Net(nn.Module): \n",
        "#     def __init__(self): \n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(1,3,3)\n",
        "#         # we use the maxpool multiple times, but define it once\n",
        "#         self.pool = nn.MaxPool2d(pooling_filter_size, pooling_stride)\n",
        "#         # in_channels = 6 because self.conv1 output 6 channel\n",
        "#         self.conv2 = nn.Conv2d(3,6,5)\n",
        "#         self.conv3 = nn.Conv2d(6,16,7)\n",
        "#         # 5*5 comes from the dimension of the last convnet layer\n",
        "#         self.fc1 = nn.Linear(16*7*7, 120) \n",
        "#         self.fc2 = nn.Linear(120, 60)\n",
        "#         self.fc3 = nn.Linear(60, 11)\n",
        "        \n",
        "#     def forward(self, x): \n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         x = self.pool(F.relu(self.conv3(x)))\n",
        "#         x = x.view(-1, 16*7*7)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = self.fc3(x)\n",
        "#         return x\n",
        "\n",
        "# net = Net().to(device)\n",
        "\n",
        "# class Net(nn.Module): \n",
        "#     def __init__(self): \n",
        "#         super().__init__()\n",
        "#         self.conv1 = nn.Conv2d(1,3,3)\n",
        "#         # we use the maxpool multiple times, but define it once\n",
        "#         self.pool = nn.MaxPool2d(pooling_filter_size, pooling_stride)\n",
        "#         # in_channels = 6 because self.conv1 output 6 channel\n",
        "#         self.conv2 = nn.Conv2d(3,6,3)\n",
        "#         self.conv3 = nn.Conv2d(6,16,5)\n",
        "#         self.conv4 = nn.Conv2d(16,26,7)\n",
        "#         # 5*5 comes from the dimension of the last convnet layer\n",
        "#         self.fc1 = nn.Linear(26*3*3, 240) \n",
        "#         self.fc2 = nn.Linear(240, 120)\n",
        "#         self.fc3 = nn.Linear(120, 60)\n",
        "#         self.fc4 = nn.Linear(60, 11)\n",
        "        \n",
        "#     def forward(self, x): \n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         x = self.pool(F.relu(self.conv3(x)))\n",
        "#         x = F.relu(self.conv4(x))\n",
        "#         x = x.view(-1, 26*3*3)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = F.relu(self.fc3(x))\n",
        "#         x = self.fc4(x)  # no activation on final layer \n",
        "#         return x\n",
        "\n",
        "# net = Net().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_R-Qon30r3j"
      },
      "source": [
        "conv1 = nn.Conv2d(1,3,5)\n",
        "pool = nn.MaxPool2d(pooling_filter_size, pooling_stride)\n",
        "conv2 = nn.Conv2d(3,6,5)\n",
        "images = real_batch[0].float()\n",
        "print(\"images\")\n",
        "print(images.shape)\n",
        "print(\"\")\n",
        "\n",
        "x = conv1(images)\n",
        "print(\"CONV1\")\n",
        "print(x.shape)\n",
        "print(\"\")\n",
        "\n",
        "x = pool(x)\n",
        "print(\"POOL1\")\n",
        "print(x.shape)\n",
        "print(\"\")\n",
        "\n",
        "x = conv2(x)\n",
        "print(\"CONV2\")\n",
        "print(x.shape)\n",
        "print(\"\")\n",
        "\n",
        "x = pool(x)\n",
        "print(\"POOL2\")\n",
        "print(x.shape)\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhDTShhOdcm7"
      },
      "source": [
        "conv1 = nn.Conv2d(1,3,3)\n",
        "pool = nn.MaxPool2d(pooling_filter_size, pooling_stride)\n",
        "conv2 = nn.Conv2d(3,6,3)\n",
        "conv3 = nn.Conv2d(6,16,5)\n",
        "conv4 = nn.Conv2d(16,26,7)\n",
        "\n",
        "images = real_batch[0].float()\n",
        "print(\"images\")\n",
        "print(images.shape)\n",
        "print(\"\")\n",
        "\n",
        "x = conv1(images)\n",
        "print(\"CONV1\")\n",
        "print(x.shape)\n",
        "\n",
        "x = pool(x)\n",
        "print(\"POOL1\")\n",
        "print(x.shape)\n",
        "print(\"\")\n",
        "\n",
        "x = conv2(x)\n",
        "print(\"CONV2\")\n",
        "print(x.shape)\n",
        "\n",
        "x = pool(x)\n",
        "print(\"POOL2\")\n",
        "print(x.shape)\n",
        "print(\"\")\n",
        "\n",
        "x = conv3(x)\n",
        "print(\"CONV3\")\n",
        "print(x.shape)\n",
        "\n",
        "x = pool(x)\n",
        "print(\"POOL3\")\n",
        "print(x.shape)\n",
        "print(\"\")\n",
        "\n",
        "x = conv4(x)\n",
        "print(\"CONV3\")\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}